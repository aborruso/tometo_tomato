[
  {
    "objectID": "doc_sources/project_review.html",
    "href": "doc_sources/project_review.html",
    "title": "Project Review: tometo_tomato",
    "section": "",
    "text": "Purpose: Python CLI to fuzzy-join CSVs using DuckDB + rapidfuzz.\nStructure: single entrypoint (tometo_tomato), core logic in src/tometo_tomato/tometo_tomato.py, tests in tests/, Quarto docs in doc_sources/ ‚Üí built site in docs/.\nPackaging: pyproject.toml + setup.py with setuptools_scm; CLI entry configured.\n\n\n\n\n\nSolid CSV ingestion: uses read_csv_auto(..., header=true, all_varchar=true) and a robust header detection with LIMIT 0 fallback.\nFlexible matching: multi-column joins, optional pair inference, and selective normalization flags (--raw-case, --raw-whitespace, --latinize, --keep-alphanumeric).\nAmbiguity handling: clear definition (ties on max avg_score), clean vs ambiguous outputs, helpful warnings.\nFuzzy fallback: tries rapidfuzz; falls back to levenshtein/damerau_levenshtein when unavailable.\nSensible CLI/UX: verbosity controls, overwrite protection via --force.\nTests present: cover headers, pair building, overwrites, whitespace/latinize.\n\n\n\n\n\nScalability: CROSS JOIN of input √ó reference in all_scores can explode on large datasets despite threshold filtering.\nSQL path quoting: some read_csv_auto('{args.input_file}') calls don‚Äôt escape single quotes; read_header() does but others should too.\nFlags vs docs mismatch: README mentions --clean-whitespace but code uses --raw-whitespace. --add-field help suggests ‚Äúspace separated‚Äù but implementation is append only.\nVersioning inconsistency: pyproject.toml sets a static version, while setuptools_scm generates src/_version.py (committed). Mixed approaches can drift.\nTest import bug: tests/test_core.py::test_scorer_option uses duckdb without importing it; test likely skips unintentionally.\nAccent normalization: strip_accents(...) used in SQL may depend on DuckDB build/extensions; not explicitly ensured/loaded.\nOutput shape: ‚Äúclean‚Äù output includes only join columns from input (not all input columns). May surprise users expecting a full left join; needs docs or an option.\n\n\n\n\n\nPerformance/blocking:\n\nUse the new --block-prefix N flag to enable prefix-based blocking and cut down candidate comparisons. Start with N=3 for city+region; adjust based on data quality.\nConsider additional lightweight prefilters (length bands, region/province exact block) before distance computation.\nConsider per-input top-1 selection without fully materializing all_scores (e.g., ORDER BY avg_score DESC LIMIT 1 per block).\n\nSQL safety:\n\nCreate a helper to quote file paths (escape single quotes) and reuse it across all read_csv_auto calls.\n\nCLI and docs alignment:\n\nReplace --clean-whitespace in README with --raw-whitespace and clarify defaults.\nFor --add-field, either implement parsing of multiple names in a single argument or adjust help to ‚Äúrepeat the flag‚Äù.\nAdd an option --all-input-columns to include all input columns in the clean output.\nValidate outputs don‚Äôt overwrite input/reference files.\n\nPackaging/versioning:\n\nAdopt setuptools_scm fully: remove static version from pyproject.toml, mark dynamic = [\"version\"], and stop committing src/_version.py.\nIf strip_accents requires an extension (e.g., ICU), INSTALL/LOAD it or document the requirement.\n\nTests/CI:\n\nFix missing import duckdb in tests/test_core.py.\nAdd tests for --infer-pairs edge thresholds, missing columns, and file paths with quotes/spaces.\nAdd ruff/black (and optionally mypy) to CI; test matrix for Python 3.8‚Äì3.12.\n\nCode hygiene:\n\nRemove unused Python UDF for latinize (logic is in SQL).\nFactor main() into smaller functions: building views, scoring expression, clean/ambiguous writers.\n\n\n\n\n\n\nsrc/tometo_tomato/tometo_tomato.py:\n\nQuote args.input_file/args.reference_file like read_header() does.\nEnsure strip_accents availability or provide a fallback/documentation.\nConsider preserving user column order in prepare_select_clauses rather than sorted sets.\n\ntests/test_core.py:\n\nAdd import duckdb at the top to ensure test_scorer_option executes as intended.\n\n\n\n\n\n\nFix README sections: duplicate ‚ÄúBasic example‚Äù, flag naming mismatch, and clarify output columns (join-only vs all input; add example and option).\nAdd a ‚ÄúPerformance Tips‚Äù section: blocking strategies, thresholds, and when to use token_set_ratio.\n\n\n\n\n\nQuick fixes: path escaping, README alignment, overwrite safeguards, test import.\nMedium: add --all-input-columns, introduce simple blocking/prefilters.\nLong: module refactor (cli/engine/io_utils), richer blocking strategies, benchmarking guidance."
  },
  {
    "objectID": "doc_sources/project_review.html#overview",
    "href": "doc_sources/project_review.html#overview",
    "title": "Project Review: tometo_tomato",
    "section": "",
    "text": "Purpose: Python CLI to fuzzy-join CSVs using DuckDB + rapidfuzz.\nStructure: single entrypoint (tometo_tomato), core logic in src/tometo_tomato/tometo_tomato.py, tests in tests/, Quarto docs in doc_sources/ ‚Üí built site in docs/.\nPackaging: pyproject.toml + setup.py with setuptools_scm; CLI entry configured."
  },
  {
    "objectID": "doc_sources/project_review.html#strengths",
    "href": "doc_sources/project_review.html#strengths",
    "title": "Project Review: tometo_tomato",
    "section": "",
    "text": "Solid CSV ingestion: uses read_csv_auto(..., header=true, all_varchar=true) and a robust header detection with LIMIT 0 fallback.\nFlexible matching: multi-column joins, optional pair inference, and selective normalization flags (--raw-case, --raw-whitespace, --latinize, --keep-alphanumeric).\nAmbiguity handling: clear definition (ties on max avg_score), clean vs ambiguous outputs, helpful warnings.\nFuzzy fallback: tries rapidfuzz; falls back to levenshtein/damerau_levenshtein when unavailable.\nSensible CLI/UX: verbosity controls, overwrite protection via --force.\nTests present: cover headers, pair building, overwrites, whitespace/latinize."
  },
  {
    "objectID": "doc_sources/project_review.html#key-risks-issues",
    "href": "doc_sources/project_review.html#key-risks-issues",
    "title": "Project Review: tometo_tomato",
    "section": "",
    "text": "Scalability: CROSS JOIN of input √ó reference in all_scores can explode on large datasets despite threshold filtering.\nSQL path quoting: some read_csv_auto('{args.input_file}') calls don‚Äôt escape single quotes; read_header() does but others should too.\nFlags vs docs mismatch: README mentions --clean-whitespace but code uses --raw-whitespace. --add-field help suggests ‚Äúspace separated‚Äù but implementation is append only.\nVersioning inconsistency: pyproject.toml sets a static version, while setuptools_scm generates src/_version.py (committed). Mixed approaches can drift.\nTest import bug: tests/test_core.py::test_scorer_option uses duckdb without importing it; test likely skips unintentionally.\nAccent normalization: strip_accents(...) used in SQL may depend on DuckDB build/extensions; not explicitly ensured/loaded.\nOutput shape: ‚Äúclean‚Äù output includes only join columns from input (not all input columns). May surprise users expecting a full left join; needs docs or an option."
  },
  {
    "objectID": "doc_sources/project_review.html#recommendations",
    "href": "doc_sources/project_review.html#recommendations",
    "title": "Project Review: tometo_tomato",
    "section": "",
    "text": "Performance/blocking:\n\nUse the new --block-prefix N flag to enable prefix-based blocking and cut down candidate comparisons. Start with N=3 for city+region; adjust based on data quality.\nConsider additional lightweight prefilters (length bands, region/province exact block) before distance computation.\nConsider per-input top-1 selection without fully materializing all_scores (e.g., ORDER BY avg_score DESC LIMIT 1 per block).\n\nSQL safety:\n\nCreate a helper to quote file paths (escape single quotes) and reuse it across all read_csv_auto calls.\n\nCLI and docs alignment:\n\nReplace --clean-whitespace in README with --raw-whitespace and clarify defaults.\nFor --add-field, either implement parsing of multiple names in a single argument or adjust help to ‚Äúrepeat the flag‚Äù.\nAdd an option --all-input-columns to include all input columns in the clean output.\nValidate outputs don‚Äôt overwrite input/reference files.\n\nPackaging/versioning:\n\nAdopt setuptools_scm fully: remove static version from pyproject.toml, mark dynamic = [\"version\"], and stop committing src/_version.py.\nIf strip_accents requires an extension (e.g., ICU), INSTALL/LOAD it or document the requirement.\n\nTests/CI:\n\nFix missing import duckdb in tests/test_core.py.\nAdd tests for --infer-pairs edge thresholds, missing columns, and file paths with quotes/spaces.\nAdd ruff/black (and optionally mypy) to CI; test matrix for Python 3.8‚Äì3.12.\n\nCode hygiene:\n\nRemove unused Python UDF for latinize (logic is in SQL).\nFactor main() into smaller functions: building views, scoring expression, clean/ambiguous writers."
  },
  {
    "objectID": "doc_sources/project_review.html#file-specific-notes",
    "href": "doc_sources/project_review.html#file-specific-notes",
    "title": "Project Review: tometo_tomato",
    "section": "",
    "text": "src/tometo_tomato/tometo_tomato.py:\n\nQuote args.input_file/args.reference_file like read_header() does.\nEnsure strip_accents availability or provide a fallback/documentation.\nConsider preserving user column order in prepare_select_clauses rather than sorted sets.\n\ntests/test_core.py:\n\nAdd import duckdb at the top to ensure test_scorer_option executes as intended."
  },
  {
    "objectID": "doc_sources/project_review.html#documentation",
    "href": "doc_sources/project_review.html#documentation",
    "title": "Project Review: tometo_tomato",
    "section": "",
    "text": "Fix README sections: duplicate ‚ÄúBasic example‚Äù, flag naming mismatch, and clarify output columns (join-only vs all input; add example and option).\nAdd a ‚ÄúPerformance Tips‚Äù section: blocking strategies, thresholds, and when to use token_set_ratio."
  },
  {
    "objectID": "doc_sources/project_review.html#suggested-next-steps",
    "href": "doc_sources/project_review.html#suggested-next-steps",
    "title": "Project Review: tometo_tomato",
    "section": "",
    "text": "Quick fixes: path escaping, README alignment, overwrite safeguards, test import.\nMedium: add --all-input-columns, introduce simple blocking/prefilters.\nLong: module refactor (cli/engine/io_utils), richer blocking strategies, benchmarking guidance."
  },
  {
    "objectID": "doc_sources/PRD.html",
    "href": "doc_sources/PRD.html",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "Integrating data from different sources often presents a common challenge: the lack of unique and consistent join keys. Typos, abbreviations, formatting variations, or simple discrepancies (e.g., ‚ÄúReggio di Calabria‚Äù, ‚ÄúReggio Calabria‚Äù, ‚ÄúReggi√≤ Calabria‚Äù) render standard SQL joins (A.key = B.key) ineffective.\nThis document defines the requirements for a ‚Äúfuzzy join‚Äù procedure that allows connecting records between two tables based on the textual similarity of fields, rather than an exact match.\n\n\n\nTo create a robust, configurable, and high-performance process for performing table joins via fuzzy string matching. The system must identify the best possible match for each record, transparently handle ambiguities, and provide clear, analysis-ready outputs.\n\n\n\n\n\n\nThe procedure allows performing a join between table A (left) and table B (right) based on the similarity of one or more pairs of text columns. New Feature: If the columns to compare (--join-pair) are not specified, the system automatically uses all columns with the same name present in both files.\n\n\n\n\nBy default:\n\nColumn matching is case insensitive (comparison ignores upper/lower case).\nMultiple and leading/trailing whitespaces are removed from each field before comparison.\n\nCLI options:\n\n--raw-case: enables case sensitive comparison (disables lower-case conversion)\n--raw-whitespace: preserves whitespaces (disables whitespace normalization)\n--latinize: normalizes accented characters and apostrophes, converting non-latin characters to latin and removing special characters (e.g.¬†‚ÄúCefal√π‚Äù and ‚ÄúCefalu‚Äô‚Äù both become ‚ÄúCefalu‚Äù)\n\nAll options can be combined as needed.\n\n\n\n\nFor each record in table A, the system calculates a similarity score (from 0 to 100) with all records in table B, using the rapidfuzz extension functions for DuckDB.\n\n\n\n\nThe system identifies the ‚Äúbest match‚Äù as the record in table B that obtains the highest similarity score.\nA join is valid only if the score exceeds a configurable minimum threshold (e.g., 85). All matches with a lower score are discarded.\n\n\n\n\nThe procedure supports joining based on multiple column pairs. If not specified, it uses all common columns.\n\n\n\n\nDefinition of ambiguity: Ambiguity occurs when a record in table A obtains the same maximum score for multiple records in table B.\nHandling: In case of ambiguity, record A and all corresponding records in B are excluded from the final join result.\nAmbiguity Output: All records excluded due to ambiguity are saved to a separate file (e.g., ambiguous_log.csv) for manual analysis.\n\n\n\n\nThe procedure produces two main outputs:\n\nClean Join Table: A file with all records from the input table (left join behavior). Records that found a unique match above the threshold will have the corresponding reference data populated; records without a match will have the reference fields empty/null.\nAmbiguity Log File: A file with records discarded due to the reasons described in FR5.\n\n\n\n\nThe system implements a left join approach:\n\nALL records from the input table are included in the clean output, regardless of whether they found a match or not.\nRecords that found a valid match (score &gt;= threshold and unambiguous) will have the reference data populated.\nRecords that did not find any match or found only matches below the threshold will have the reference fields empty/null.\nThis allows users to study and analyze which records were not successfully joined.\n\n\n\n\n\n\n\nThe procedure is optimized to handle large datasets. The use of WHERE score &gt; threshold in DuckDB reduces computational load. Additionally, optional blocking via --block-prefix N restricts candidate comparisons to records sharing the same prefix-based block key, drastically reducing comparisons while preserving accuracy on typical city/region data.\n\n\n\nThe user can easily configure: - Input file paths. - Column names to use for the join (optional; if omitted, common columns are used). - Similarity threshold (number from 0 to 100). - rapidfuzz function to use (e.g., rapidfuzz_ratio, rapidfuzz_token_sort_ratio). - Output file paths. - Optional blocking strength via --block-prefix.\n\n\n\nThe process produces an execution log with key statistics: number of input records, successful joins, ambiguous cases.\n\n\n\n\n\nProcessing Engine: DuckDB\nFuzzy Matching Library: rapidfuzz extension for DuckDB\nOrchestration: Python script (single-command CLI: tometo_tomato)\n\n\n\n\nThe project documentation will be published using Quarto, leveraging the project‚Äôs docs folder.\n\n\n\nThis use case demonstrates the association of ISTAT codes with an unofficial registry, managing inaccuracies in place names.\nTable A (ref.csv - Official ISTAT Source) Contains official data of Italian municipalities.\n\n\n\nregion\nmunicipality\nmunicipality_code\n\n\n\n\nCalabria\nReggio Calabria\n80065\n\n\nLombardy\nMilan\n015146\n\n\nPiedmont\nTurin\n001272\n\n\nLazio\nRome\n058091\n\n\nCampania\nNaples\n063049\n\n\n\nTable B (input.csv - Unofficial Registry) Contains data with possible typos.\n\n\n\nregion\nmunicipality\n\n\n\n\nCalabria\nReggio Calabr\n\n\nLombardy\nMilan\n\n\nPiedmont\nTorinoo\n\n\nLazio\nRma\n\n\nCampania\nNaples\n\n\n\nObjective Associate the municipality_code from Table A (ref.csv) with records in Table B (input.csv).\nConfiguration (CLI Call Example) The process is executed via the single-command CLI tometo_tomato:\ntometo_tomato input.csv ref.csv --join-pair region,region --join-pair municipality,municipality --add-field municipality_code --threshold 90 --show-score\nOr, if the columns to compare coincide in the two files:\ntometo_tomato input.csv ref.csv --add-field municipality_code --threshold 90 --show-score\nThe process identifies the best match for each row in input.csv within ref.csv and associates the corresponding municipality_code. All input records are included in the output (left join behavior).\nExample of expected matches:\n\ninput.csv (Reggio Calabr, Calabria) -&gt; ref.csv (Reggio Calabria, Calabria) with municipality_code 80065.\ninput.csv (Torinoo, Piedmont) -&gt; ref.csv (Turin, Piedmont) with municipality_code 001272.\ninput.csv (Rma, Lazio) -&gt; ref.csv (Rome, Lazio) with municipality_code 058091.\nRecords with no match or low similarity scores will have empty/null values for municipality_code but will still appear in the output.\nThe final result is a table with all rows from input.csv plus the associated municipality_code (populated only for successful matches)."
  },
  {
    "objectID": "doc_sources/PRD.html#introduction",
    "href": "doc_sources/PRD.html#introduction",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "Integrating data from different sources often presents a common challenge: the lack of unique and consistent join keys. Typos, abbreviations, formatting variations, or simple discrepancies (e.g., ‚ÄúReggio di Calabria‚Äù, ‚ÄúReggio Calabria‚Äù, ‚ÄúReggi√≤ Calabria‚Äù) render standard SQL joins (A.key = B.key) ineffective.\nThis document defines the requirements for a ‚Äúfuzzy join‚Äù procedure that allows connecting records between two tables based on the textual similarity of fields, rather than an exact match.\n\n\n\nTo create a robust, configurable, and high-performance process for performing table joins via fuzzy string matching. The system must identify the best possible match for each record, transparently handle ambiguities, and provide clear, analysis-ready outputs."
  },
  {
    "objectID": "doc_sources/PRD.html#functional-requirements",
    "href": "doc_sources/PRD.html#functional-requirements",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "The procedure allows performing a join between table A (left) and table B (right) based on the similarity of one or more pairs of text columns. New Feature: If the columns to compare (--join-pair) are not specified, the system automatically uses all columns with the same name present in both files.\n\n\n\n\nBy default:\n\nColumn matching is case insensitive (comparison ignores upper/lower case).\nMultiple and leading/trailing whitespaces are removed from each field before comparison.\n\nCLI options:\n\n--raw-case: enables case sensitive comparison (disables lower-case conversion)\n--raw-whitespace: preserves whitespaces (disables whitespace normalization)\n--latinize: normalizes accented characters and apostrophes, converting non-latin characters to latin and removing special characters (e.g.¬†‚ÄúCefal√π‚Äù and ‚ÄúCefalu‚Äô‚Äù both become ‚ÄúCefalu‚Äù)\n\nAll options can be combined as needed.\n\n\n\n\nFor each record in table A, the system calculates a similarity score (from 0 to 100) with all records in table B, using the rapidfuzz extension functions for DuckDB.\n\n\n\n\nThe system identifies the ‚Äúbest match‚Äù as the record in table B that obtains the highest similarity score.\nA join is valid only if the score exceeds a configurable minimum threshold (e.g., 85). All matches with a lower score are discarded.\n\n\n\n\nThe procedure supports joining based on multiple column pairs. If not specified, it uses all common columns.\n\n\n\n\nDefinition of ambiguity: Ambiguity occurs when a record in table A obtains the same maximum score for multiple records in table B.\nHandling: In case of ambiguity, record A and all corresponding records in B are excluded from the final join result.\nAmbiguity Output: All records excluded due to ambiguity are saved to a separate file (e.g., ambiguous_log.csv) for manual analysis.\n\n\n\n\nThe procedure produces two main outputs:\n\nClean Join Table: A file with all records from the input table (left join behavior). Records that found a unique match above the threshold will have the corresponding reference data populated; records without a match will have the reference fields empty/null.\nAmbiguity Log File: A file with records discarded due to the reasons described in FR5.\n\n\n\n\nThe system implements a left join approach:\n\nALL records from the input table are included in the clean output, regardless of whether they found a match or not.\nRecords that found a valid match (score &gt;= threshold and unambiguous) will have the reference data populated.\nRecords that did not find any match or found only matches below the threshold will have the reference fields empty/null.\nThis allows users to study and analyze which records were not successfully joined."
  },
  {
    "objectID": "doc_sources/PRD.html#non-functional-requirements",
    "href": "doc_sources/PRD.html#non-functional-requirements",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "The procedure is optimized to handle large datasets. The use of WHERE score &gt; threshold in DuckDB reduces computational load. Additionally, optional blocking via --block-prefix N restricts candidate comparisons to records sharing the same prefix-based block key, drastically reducing comparisons while preserving accuracy on typical city/region data.\n\n\n\nThe user can easily configure: - Input file paths. - Column names to use for the join (optional; if omitted, common columns are used). - Similarity threshold (number from 0 to 100). - rapidfuzz function to use (e.g., rapidfuzz_ratio, rapidfuzz_token_sort_ratio). - Output file paths. - Optional blocking strength via --block-prefix.\n\n\n\nThe process produces an execution log with key statistics: number of input records, successful joins, ambiguous cases."
  },
  {
    "objectID": "doc_sources/PRD.html#technology-stack",
    "href": "doc_sources/PRD.html#technology-stack",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "Processing Engine: DuckDB\nFuzzy Matching Library: rapidfuzz extension for DuckDB\nOrchestration: Python script (single-command CLI: tometo_tomato)"
  },
  {
    "objectID": "doc_sources/PRD.html#documentation",
    "href": "doc_sources/PRD.html#documentation",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "The project documentation will be published using Quarto, leveraging the project‚Äôs docs folder."
  },
  {
    "objectID": "doc_sources/PRD.html#example-use-case-istat-code-association",
    "href": "doc_sources/PRD.html#example-use-case-istat-code-association",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "This use case demonstrates the association of ISTAT codes with an unofficial registry, managing inaccuracies in place names.\nTable A (ref.csv - Official ISTAT Source) Contains official data of Italian municipalities.\n\n\n\nregion\nmunicipality\nmunicipality_code\n\n\n\n\nCalabria\nReggio Calabria\n80065\n\n\nLombardy\nMilan\n015146\n\n\nPiedmont\nTurin\n001272\n\n\nLazio\nRome\n058091\n\n\nCampania\nNaples\n063049\n\n\n\nTable B (input.csv - Unofficial Registry) Contains data with possible typos.\n\n\n\nregion\nmunicipality\n\n\n\n\nCalabria\nReggio Calabr\n\n\nLombardy\nMilan\n\n\nPiedmont\nTorinoo\n\n\nLazio\nRma\n\n\nCampania\nNaples\n\n\n\nObjective Associate the municipality_code from Table A (ref.csv) with records in Table B (input.csv).\nConfiguration (CLI Call Example) The process is executed via the single-command CLI tometo_tomato:\ntometo_tomato input.csv ref.csv --join-pair region,region --join-pair municipality,municipality --add-field municipality_code --threshold 90 --show-score\nOr, if the columns to compare coincide in the two files:\ntometo_tomato input.csv ref.csv --add-field municipality_code --threshold 90 --show-score\nThe process identifies the best match for each row in input.csv within ref.csv and associates the corresponding municipality_code. All input records are included in the output (left join behavior).\nExample of expected matches:\n\ninput.csv (Reggio Calabr, Calabria) -&gt; ref.csv (Reggio Calabria, Calabria) with municipality_code 80065.\ninput.csv (Torinoo, Piedmont) -&gt; ref.csv (Turin, Piedmont) with municipality_code 001272.\ninput.csv (Rma, Lazio) -&gt; ref.csv (Rome, Lazio) with municipality_code 058091.\nRecords with no match or low similarity scores will have empty/null values for municipality_code but will still appear in the output.\nThe final result is a table with all rows from input.csv plus the associated municipality_code (populated only for successful matches)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tometo Tomato",
    "section": "",
    "text": "Your data has typos. Your reference file doesn‚Äôt. Let‚Äôs fix that.\nTometo Tomato is a command-line tool that connects two CSV files by similarity, not just exact matches. It handles typos, abbreviations, accents, and formatting differences ‚Äî so you can enrich your messy data using an authoritative reference table.\nBuilt on DuckDB and rapidfuzz."
  },
  {
    "objectID": "doc_sources/install.html",
    "href": "doc_sources/install.html",
    "title": "Installation",
    "section": "",
    "text": "Make sure you have Python (version 3.8 or higher) installed.\nFor efficient package management, we recommend uv ‚Äî a modern and fast tool for managing Python environments."
  },
  {
    "objectID": "doc_sources/install.html#prerequisiti",
    "href": "doc_sources/install.html#prerequisiti",
    "title": "Installazione",
    "section": "Prerequisiti",
    "text": "Prerequisiti\nAssicurati di avere installato Python (versione 3.8 o superiore). Per una gestione efficiente dei pacchetti, raccomandiamo l‚Äôuso di uv, un tool moderno e veloce."
  },
  {
    "objectID": "doc_sources/install.html#metodo-1-installazione-per-sviluppo-da-sorgente",
    "href": "doc_sources/install.html#metodo-1-installazione-per-sviluppo-da-sorgente",
    "title": "Installazione",
    "section": "Metodo 1: Installazione per Sviluppo (da sorgente)",
    "text": "Metodo 1: Installazione per Sviluppo (da sorgente)\nQuesto metodo √® ideale se intendi contribuire al progetto o modificarne il codice.\n\nClona il repository: bash     git clone https://github.com/aborruso/tometo_tomato.git\nNaviga nella directory del progetto: bash     cd tometo_tomato\nInstalla le dipendenze:\n\nCon uv (raccomandato): bash     uv pip install -e . Questo installer√† il pacchetto in modalit√† ‚Äúeditabile‚Äù, permettendoti di vedere immediatamente le modifiche al codice senza dover reinstallare.\nCon pip: bash     pip install -e .\n\nSe non intendi modificare il codice, puoi omettere il flag -e:\n\nuv pip install .\npip install ."
  },
  {
    "objectID": "doc_sources/install.html#metodo-2-installazione-come-tool-globale-con-uv-tool-install",
    "href": "doc_sources/install.html#metodo-2-installazione-come-tool-globale-con-uv-tool-install",
    "title": "Installazione",
    "section": "Metodo 2: Installazione come Tool Globale (con uv tool install)",
    "text": "Metodo 2: Installazione come Tool Globale (con uv tool install)\nSe vuoi semplicemente usare tometo_tomato come un comando da riga di comando senza preoccuparti della gestione del codice sorgente o degli ambienti virtuali, uv tool install √® la soluzione pi√π pulita.\n\nAssicurati di avere uv installato.\nInstalla il tool: bash     uv tool install tometo_tomato Questo comando installer√† tometo_tomato in un ambiente isolato gestito da uv e lo render√† disponibile nel tuo PATH."
  },
  {
    "objectID": "doc_sources/install.html#verifica-dellinstallazione",
    "href": "doc_sources/install.html#verifica-dellinstallazione",
    "title": "Installazione",
    "section": "Verifica dell‚ÄôInstallazione",
    "text": "Verifica dell‚ÄôInstallazione\nDopo aver completato uno dei metodi di installazione, puoi verificare che tometo_tomato sia correttamente installato eseguendo:\ntometo_tomato --version\nDovresti vedere la versione del tool stampata a schermo."
  },
  {
    "objectID": "doc_sources/about.html",
    "href": "doc_sources/about.html",
    "title": "About Me",
    "section": "",
    "text": "This is the about page.\nYou can put information about yourself or your project here."
  },
  {
    "objectID": "doc_sources/how-to.html",
    "href": "doc_sources/how-to.html",
    "title": "Come usare Tom√©to Tomato",
    "section": "",
    "text": "tometo_tomato √® un‚Äôutilit√† a riga di comando progettata per eseguire ‚Äúfuzzy join‚Äù tra due file CSV. In altre parole, permette di collegare due tabelle basandosi su colonne i cui valori non sono identici, ma semplicemente ‚Äúsimili‚Äù, gestendo errori di battitura, abbreviazioni o differenze di formattazione."
  },
  {
    "objectID": "doc_sources/how-to.html#introduzione",
    "href": "doc_sources/how-to.html#introduzione",
    "title": "Come usare Tom√©to Tomato",
    "section": "",
    "text": "tometo_tomato √® un‚Äôutilit√† a riga di comando progettata per eseguire ‚Äúfuzzy join‚Äù tra due file CSV. In altre parole, permette di collegare due tabelle basandosi su colonne i cui valori non sono identici, ma semplicemente ‚Äúsimili‚Äù, gestendo errori di battitura, abbreviazioni o differenze di formattazione."
  },
  {
    "objectID": "doc_sources/how-to.html#il-concetto-di-base",
    "href": "doc_sources/how-to.html#il-concetto-di-base",
    "title": "Come usare Tom√©to Tomato",
    "section": "Il Concetto di Base",
    "text": "Il Concetto di Base\nL‚Äôidea √® semplice: 1. Hai un file di input con dati ‚Äúsporchi‚Äù o incompleti. 2. Hai un file di riferimento (anagrafica) che contiene i dati ‚Äúpuliti‚Äù e corretti. 3. Vuoi arricchire il file di input aggiungendo colonne prese dal file di riferimento, trovando le corrispondenze migliori anche quando non sono perfette."
  },
  {
    "objectID": "doc_sources/how-to.html#i-nostri-file-di-esempio",
    "href": "doc_sources/how-to.html#i-nostri-file-di-esempio",
    "title": "Come usare Tom√©to Tomato",
    "section": "I Nostri File di Esempio",
    "text": "I Nostri File di Esempio\nPer questa guida, useremo due file:\n1. File di input (docs/files/input.csv)\nUn file con un elenco di comuni e regioni, ma con nomi imprecisi e problemi di case.\ncity,region\nCefalu',Sicilia\nReggio Calabria,CALABRIA\nRODENGO-SAIANO,Lombardia\n2. File di riferimento (docs/files/ref.csv)\nUn‚Äôanagrafica pulita che contiene anche il codice ISTAT (city_code) che vogliamo aggiungere al nostro file di input.\ncity,region,city_code\nCefal√π,Sicilia,082027\nReggio di Calabria,Calabria,080063\nRodengo Saiano,Lombardia,017164"
  },
  {
    "objectID": "doc_sources/how-to.html#uso-fondamentale",
    "href": "doc_sources/how-to.html#uso-fondamentale",
    "title": "Come usare Tom√©to Tomato",
    "section": "Uso Fondamentale",
    "text": "Uso Fondamentale\nPer collegare questi due file, useremo il seguente comando:\ntometo_tomato docs/files/input.csv docs/files/ref.csv \\\n  -j \"city,city\" \\\n  -j \"region,region\" \\\n  -a \"city_code\" \\\n  -s \\\n  -o \"output/clean_matches.csv\"\nAnalizziamo il comando pezzo per pezzo:\n\ntometo_tomato docs/files/input.csv docs/files/ref.csv: Indichiamo il programma da eseguire, seguito dal file di input e da quello di riferimento.\n-j \"city,city\": Specifichiamo la prima coppia di colonne da usare per il join. In questo caso i nomi delle colonne sono identici.\n-j \"region,region\": Specifichiamo la seconda coppia di colonne. Il programma calcoler√† un punteggio di somiglianza medio basato su tutte le coppie fornite.\n-a \"city_code\": Indichiamo quale colonna del file di riferimento (ref.csv) vogliamo aggiungere al nostro file di output.\n-s: Chiediamo di includere anche una colonna con il punteggio di somiglianza (avg_score) nell‚Äôoutput.\n-o \"output/clean_matches.csv\": Specifichiamo il percorso del file CSV di output in cui salvare i risultati."
  },
  {
    "objectID": "doc_sources/how-to.html#il-risultato",
    "href": "doc_sources/how-to.html#il-risultato",
    "title": "Come usare Tom√©to Tomato",
    "section": "Il Risultato",
    "text": "Il Risultato\nDopo aver eseguito il comando, il file output/clean_matches.csv conterr√† i dati del file di input, arricchiti con le colonne del file di riferimento per le righe che hanno trovato una corrispondenza con un punteggio sufficientemente alto. Ad esempio, la riga RODENGO-SAIANO,Lombardia verr√† collegata a Rodengo Saiano,Lombardia, e il city_code 017164 verr√† aggiunto alla riga di output."
  },
  {
    "objectID": "doc_sources/how-to.html#opzioni-avanzate-utili",
    "href": "doc_sources/how-to.html#opzioni-avanzate-utili",
    "title": "Come usare Tom√©to Tomato",
    "section": "Opzioni Avanzate Utili",
    "text": "Opzioni Avanzate Utili\nL‚Äôutilit√† offre molte altre opzioni per personalizzare il processo:\n\n--threshold o -t: Permette di modificare la soglia di somiglianza minima (default: 85) per considerare una corrispondenza valida.\n--infer-pairs: Tenta di indovinare automaticamente le coppie di colonne da usare per il join, basandosi sulla somiglianza dei nomi delle colonne.\n--latinize: Normalizza i caratteri, rimuovendo accenti e simboli speciali prima del confronto, come abbiamo visto nei nostri esempi con strip_accents.\n--keep-alphanumeric o -k: Mantiene solo lettere, numeri e spazi nelle colonne di join, rimuovendo punteggiatura e caratteri speciali. Utile per dati con apostrofi, trattini, punti, ecc.\n\nEsempio: bash   tometo_tomato input.csv ref.csv -j \"name,ref_name\" --keep-alphanumeric -o output.csv   # oppure versione breve   tometo_tomato input.csv ref.csv -j \"name,ref_name\" -k -o output.csv Questo permette di far corrispondere, ad esempio, John O'Connor con John OConnor, oppure Anna & Co. con Anna Co.\n\n--raw-case e --raw-whitespace: Disabilitano la normalizzazione di maiuscole/minuscole e degli spazi, per un controllo pi√π fine.\n\n\nPrestazioni: Blocking con --block-prefix\nSu dataset grandi, confrontare ogni riga dell‚Äôinput con ogni riga del riferimento √® costoso. Con --block-prefix N il confronto avviene solo tra record che condividono la stessa chiave di blocco, costruita concatenando i primi N caratteri delle colonne di join gi√† normalizzate.\nEsempio (prefisso di 3 caratteri su city e region):\ntometo_tomato docs/files/input.csv docs/files/ref.csv \\\n  -j \"city,city\" -j \"region,region\" \\\n  -a \"city_code\" -s -t 85 \\\n  --block-prefix 3 \\\n  -o \"output/clean_matches.csv\"\nSuggerimenti per N: - 2‚Äì3: pi√π sicuro su dati ‚Äúsporchi‚Äù (abbreviazioni, apostrofi) - 3: buon default per city+region - 4+: pruning pi√π forte su dati puliti; valuta multi‚Äëpass se cala la recall\nNota: la chiave usa le stesse normalizzazioni del matching (spazi, case, --latinize, --keep-alphanumeric)."
  },
  {
    "objectID": "doc_sources/how-to.html#ambiguit√†-e-comportamento-degli-output",
    "href": "doc_sources/how-to.html#ambiguit√†-e-comportamento-degli-output",
    "title": "Come usare Tom√©to Tomato",
    "section": "Ambiguit√† e comportamento degli output",
    "text": "Ambiguit√† e comportamento degli output\nLo strumento segnala e gestisce in modo esplicito i casi ambigui per evitare di inserire dati potenzialmente errati nell‚Äôoutput ‚Äúpulito‚Äù.\n\nDefinizione di ambiguit√†: una riga di input √® considerata ‚Äúambigua‚Äù quando due o pi√π righe del file di riferimento ottengono lo stesso punteggio massimo medio (avg_score) per quell‚Äôinput, e tale punteggio soddisfa la soglia --threshold.\nComportamento: le righe ambigue NON vengono incluse nell‚Äô--output-clean. In questo modo il file ‚Äúpulito‚Äù contiene solo corrispondenze non ambigue (una sola migliore corrispondenza con punteggio &gt;= soglia).\nEsportazione ambigui: per salvare le righe candidate ambigue, usare l‚Äôopzione --output-ambiguous ambiguous.csv. Il file conterr√† le righe di riferimento che hanno ottenuto il punteggio massimo (uguale) per ciascun input ambiguo.\nAvviso: lo strumento rileva sempre la presenza di record ambigui e mostra un avviso quando ne trova uno; l‚Äôavviso suggerisce di usare --output-ambiguous per salvare i dettagli.\n\nEsempio: se per l‚Äôinput \"Reggio Calabria\" due righe di riferimento diverse ottengono entrambe 95.45 e la soglia √® 85, l‚Äôinput risulter√† ambiguo, non comparir√† in --output-clean e potrai ispezionare entrambe le righe candidate con --output-ambiguous."
  },
  {
    "objectID": "doc_sources/introduction.html",
    "href": "doc_sources/introduction.html",
    "title": "Why Fuzzy Matching?",
    "section": "",
    "text": "Anyone who works with data regularly encounters columns that should follow a standard encoding ‚Äî an official list of values ‚Äî but instead contain typos, extra whitespace, special characters instead of accented letters, inconsistent capitalization, and so on.\nHere are some Italian city names, written incorrectly:\n\nExamples of incorrect city names\n\n\n\n\n\n\ncity\ntype of error\n\n\n\n\nCefalu‚Äô\nshould use √π instead of u'\n\n\nReggio Calabria\nthe official name is Reggio di Calabria\n\n\nRODENGO-SAIANO\nshould be Rodengo Saiano, without - and not all uppercase\n\n\n\n If you wanted to associate each city with the code1 that ISTAT ‚Äî the Italian National Institute of Statistics ‚Äî assigns to each municipality, a simple exact join would fail because the names don‚Äôt match. Associating a code to each municipality is very important: it enables you to link data from different sources that would otherwise be incomparable, and to automatically generate maps, since mapping software often uses these codes to identify municipalities.\n\n\n\n\n\n\nPay Attention\n\n\n\nData educators usually shout: CODES, NOT LABELS!!\n\n\nThe codes for Italian administrative units are public and freely available under CC-BY-4.0 on various sections of the ISTAT website. One source is SITUAS, which has a page with the ‚ÄúList of codes and names of territorial units‚Äù, downloadable in CSV and JSON format.\n\nOfficial ISTAT municipality data\n\n\ncity\nregion\nistat_city_code\n\n\n\n\nCefal√π\nSicilia\n082027\n\n\nReggio di Calabria\nCalabria\n080063\n\n\nRodengo Saiano\nLombardia\n017163\n\n\n\n If you try to join the incorrect data with this official data, you get no results.\n\nTometo Tomato was created to solve this problem: it lets you easily perform joins that are not based on exact matches, but on similarity, and then enrich, modify, and correct your messy input table by comparing it against a reference table.\n\n\n\nThese are our two sample tables, available as input.csv and ref_sample.csv so you can download them and try the examples yourself."
  },
  {
    "objectID": "doc_sources/introduction.html#introduzione",
    "href": "doc_sources/introduction.html#introduzione",
    "title": "Perch√© √® stato creato Tom√©to Tomato",
    "section": "",
    "text": "Chiunque lavori con i dati molto spesso si imbatte in colonne che dovrebbero rispettare una codifica, un valore standard, una lista di valori noti, ma presentano invece errori di battitura, spazi in eccesso, caratteri speciali al posto di caratteri accentati, maiuscole/minuscole non coerenti, ecc..\nQui sotto ad esempio dei nomi ci citt√† italiane, riportati in modo errato:\n\nEsempi di nomi di citt√† errati\n\n\n\n\n\n\ncity\ntipo di errore\n\n\n\n\nCefalu‚Äô\nbisognerebbe usare la √π e non u'\n\n\nReggio Calabria\nIl nome corretto √® Reggio di Calabria\n\n\nRODENGO-SAIANO\nbisognerebbe usare Rodengo Saiano, senza - e non tutto maiuscolo\n\n\n\n Se volessi associare a queste citt√† il codice1 che Istat - l‚Äôistituto nazionale di statistica - assegna a ciascuna citt√†, non potrei farlo con una semplice operazione di join, perch√© i nomi non corrispondono esattamente. Associare un codice a ciascun comune √® operazione molto importante perch√© mi consente di unire dati provenienti da fonti diverse, che altrimenti non potrei confrontare, ma anche di generare ad esempio automaticamente mappe, perch√© i software spesso usano proprio questi codici per identificare i comuni.\n\n\n\n\n\n\nPay Attention\n\n\n\nChi fa didattica sui dati di solito infatti dice (urlando): CODES, NOT LABELS!!\n\n\nI codici correlati alle unit√† amministrative italiane sono pubblici e liberamente accessibili in CC-BY-4.0 su diverse sezioni del sito Istat. Uno di questi √® il SITUAS, in cui c‚Äô√® la pagina con l‚Äô‚ÄúElenco dei codici e delle denominazioni delle unit√† territoriali‚Äù, scaricabili in formato CSV e JSON.\n\nEsempio di dati ufficiali Istat sui comuni italiani\n\n\ncity\nregion\nistat_city_code\n\n\n\n\nCefal√π\nSicilia\n082027\n\n\nReggio di Calabria\nCalabria\n080063\n\n\nRodengo Saiano\nLombardia\n017163\n\n\n\n Se provassi a fare un join tra i dati errati e questi dati ufficiali, non otterrei alcun risultato.\n\nüëâ Tom√©to Tomato √® stato creato per risolvere questo problema: consente di fare comodamente join non basati su corrispondenze esatte, ma su corrispondenze ‚Äúsimili‚Äù, e arricchire, modificare, correggere la propria brutta tabella di input, confrontandola con una tabella di riferimento.\n\n\n\nQueste sono le nostre due tabelle di esempio, disponibili come input.csv e ref_sample.csv in modo che sia possibile scaricarle e provare a eseguire gli esempi."
  },
  {
    "objectID": "doc_sources/introduction.html#utilizzare-sql",
    "href": "doc_sources/introduction.html#utilizzare-sql",
    "title": "Perch√© √® stato creato Tom√©to Tomato",
    "section": "Utilizzare SQL",
    "text": "Utilizzare SQL\n\nFare il JOIN\nIl primo test √® quello di lanciare una semplice query SQL di join, per vedere cosa succede a partire dai nostri dati di esempio.\n\n\n\nThe raw data\n\n\ncity\nregion\n\n\n\n\nCefalu‚Äô\nSicilia\n\n\nReggio Calabria\nCALABRIA\n\n\nRODENGO-SAIANO\nLombardia\n\n\n\n\n\n\n\nThe reference data\n\n\ncity\nregion\ncity_code\n\n\n\n\nCefal√π\nSicilia\n082027\n\n\nReggio di Calabria\nCalabria\n080063\n\n\nRodengo Saiano\nLombardia\n017164\n\n\n\n\n\n La query pu√≤ essere quella di sotto. √à impostata come un LEFT JOIN, in modo da mostrare tutte le righe della tabella di sinistra, anche se non trovano corrispondenza nella tabella di destra.\nSELECT\n  i.*,\n  r.city_code\nFROM read_csv_auto('input.csv') AS i\nLEFT JOIN read_csv_auto('ref_sample.csv') AS r\n  ON i.city = r.city AND i.region = r.region\n\n\n\n\n\n\nNote\n\n\n\nNota: l‚Äôuso di read_csv_auto nella query di sopra √® una funzionalit√† di DuckDB che consente di leggere direttamente file CSV senza doverli importare prima in una tabella. In questo modo √® possibile fare esperimenti veloci senza dover creare tabelle temporanee.\n\n\nIn output otteniamo un pessimo risultato: nessuna delle righe della tabella di sinistra trova corrispondenza nella tabella di destra, e quindi il campo city_code risulta sempre NULL.\n\nRisultato di un join tra dati errati e dati ufficiali\n\n\ncity\nregion\nistat_city_code\n\n\n\n\nCefalu‚Äô\nSicilia\nNULL\n\n\nReggio Calabria\nCALABRIA\nNULL\n\n\nRODENGO-SAIANO\nLombardia\nNULL\n\n\n\n\n\nFare il JOIN ‚Äúfuzzy‚Äù\nUn JOIN ‚Äúfuzzy‚Äù, sfumato, √® quello che consente di trovare corrispondenze anche quando i valori non sono esattamente uguali, ma ‚Äúsimili‚Äù. Ad esempio, potremmo voler considerare come corrispondenti i nomi di citt√† che differiscono per un solo carattere, o che hanno una distanza di Levenshtein (numero di operazioni necessarie per trasformare una stringa in un‚Äôaltra) inferiore a una certa soglia.\nPotremmo riscrivere la query di join precedente in questo modo, usando la funzione levenshtein di DuckDB per il campo city in modo da considererare come corrispondenti i nomi di citt√† che differiscono per al massimo 2 caratteri:\nSELECT\n  i.city AS input_city,\n  i.region AS input_region,\n  r.city AS ref_city,\n  r.region AS ref_region,\n  r.city_code,\n  levenshtein(i.city, r.city) AS levenshtein_distance\nFROM read_csv_auto('input.csv') AS i\nJOIN read_csv_auto('ref_sample.csv') AS r\n  ON i.region = r.region\n  AND levenshtein(i.city, r.city) &lt;= 2;\n\nRisultato di un join fuzzy tra dati errati e dati ufficiali\n\n\n\n\n\n\n\n\n\n\ninput_city\ninput_region\nref_city\nref_region\ncity_code\nlevenshtein_distance\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n2\n\n\n\nL‚Äôunica citt√† di cui c‚Äô√® un risulato √® soltanto Cefalu', perch√© la distanza di Levenshtein tra Cefalu' e Cefal√π √® 2: sostituzione di u' con √π e rimozione di '.\nSe aumentiamo la soglia a 10, non cambia nulla, perch√© ad esempio il Comune di Rodengo Saiano √® scritto in maiuscolo e con il trattino e la distanza tra RODENGO-SAIANO e Rodengo Saiano √® pari a 12:\nSELECT levenshtein('RODENGO-SAIANO', 'Rodengo Saiano') AS distance;\n\n12\nSe si imposta la soglia a 15 ne otteniamo in ogni caso soltanto 2, perch√© il JOIN del campo region non trova corrispondenza tra CALABRIA e Calabria. Quindi dovremmo usare una funzione di distanza tra stringhe anche per il campo region:\nSELECT\n  i.city AS input_city,\n  i.region AS input_region,\n  r.city AS ref_city,\n  r.region AS ref_region,\n  r.city_code,\n  levenshtein(i.city, r.city) AS levenshtein_distance\nFROM read_csv_auto('input.csv') AS i\nJOIN read_csv_auto('ref_sample.csv') AS r\n  ON levenshtein(i.city, r.city) &lt;= 15\n  AND levenshtein(LOWER(i.region), LOWER(r.region)) &lt; 10\nMa √® l‚Äôoutput non √® proprio quello che ci aspettiamo, non 3 righe in totale (una per ogni Comune), ma ben 8 righe:\n\nRisultato di un join fuzzy tra dati errati e dati ufficiali\n\n\n\n\n\n\n\n\n\n\ninput_city\ninput_region\nref_city\nref_region\ncity_code\nlevenshtein_distance\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n2\n\n\nCefalu‚Äô\nSicilia\nReggio di Calabria\nCalabria\n080063\n15\n\n\nCefalu‚Äô\nSicilia\nRodengo Saiano\nLombardia\n017163\n12\n\n\nReggio Calabria\nCALABRIA\nCefal√π\nSicilia\n082027\n12\n\n\nReggio Calabria\nCALABRIA\nReggio di Calabria\nCalabria\n080063\n3\n\n\nReggio Calabria\nCALABRIA\nRodengo Saiano\nLombardia\n017163\n10\n\n\nRODENGO-SAIANO\nLombardia\nCefal√π\nSicilia\n082027\n14\n\n\nRODENGO-SAIANO\nLombardia\nRodengo Saiano\nLombardia\n017163\n12\n\n\n\nQuando si esegue un JOIN esatto, l‚Äôobiettivo √® trovare una singola e chiara corrispondenza per ogni riga. Nel mondo del ‚Äúfuzzy matching‚Äù, le regole cambiano. Abbassando le nostre pretese con soglie di distanza permissive, non stiamo pi√π chiedendo al database ‚Äútrova l‚Äôunica corrispondenza giusta‚Äù, ma piuttosto:\n\nPer ogni riga del mio input, trovami tutte le righe nel file di riferimento che soddisfano questi criteri di somiglianza generici.\n\nSe una riga di input √® ‚Äúvagamente simile‚Äù a pi√π righe di riferimento, il database creer√† una riga di output per ciascuna di queste coincidenze. Questo effetto di moltiplicazione √® noto come prodotto cartesiano delle coincidenze.\nDopo aver generato le possibili corrispondenze, dovremmo filtrarle per tenere solo la migliore per ogni record di partenza. Il processo pu√≤ essere suddiviso in tre fasi:\n\ntrovare tutte le possibili corrispondenze e calcolare le distanze\nassegnare un rango a ciascuna corrispondenza in base alla somma delle distanze\nselezionare solo la corrispondenza con il rango pi√π alto (cio√® la pi√π simile)\n\n-- Fase 1: trova tutti i candidati e calcola le distanze\nWITH all_matches AS (\n    SELECT\n        i.city AS input_city,\n        i.region AS input_region,\n        r.city AS ref_city,\n        r.region AS ref_region,\n        r.city_code,\n        levenshtein(i.city, r.city) AS city_distance,\n        levenshtein(i.region, r.region) AS region_distance\n    FROM read_csv_auto('input.csv') AS i\n    JOIN read_csv_auto('ref_sample.csv') AS r\n      ON levenshtein(i.city, r.city) &lt;= 15\n     AND levenshtein(i.region, r.region) &lt; 10\n),\n-- Fase 2: assegna un rango ai candidati\nranked_matches AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (\n            PARTITION BY input_city, input_region\n            ORDER BY (city_distance + region_distance) ASC\n        ) AS rn\n    FROM all_matches\n)\n-- Fase 3: prendi solo il miglior candidato (rn = 1)\nSELECT\n    input_city,\n    input_region,\n    ref_city,\n    ref_region,\n    city_code,\n    city_distance,\n    region_distance\nFROM ranked_matches\nWHERE rn = 1;\n\nRisultato di un join fuzzy, con selezione della migliore corrispondenza\n\n\n\n\n\n\n\n\n\n\n\ninput_city\ninput_region\nref_city\nref_region\ncity_code\ncity_distance\nregion_distance\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n2\n0\n\n\nRODENGO-SAIANO\nLombardia\nRodengo Saiano\nLombardia\n017163\n12\n0\n\n\nReggio Calabria\nCALABRIA\nReggio di Calabria\nCalabria\n080063\n3\n7\n\n\n\n\n\nUna sintesi per ridurre le distanze\nA seguira una tabellina di riepilogo, sull‚Äôesempio del nome del Comune di Forza¬∑d'Agr√≤, scritto per√≤ in modo errato Forza¬∑¬∑D‚ÄôAgro¬∑:\n\nc‚Äô√® la D maiuscola invece che minuscola;\nci sono due spazi in eccesso (uno tra Forza e D'Agro e uno alla fine);\nnon c‚Äô√® la o accentata, ma una o normale;\nc‚Äô√® un apostrofo tipografico ‚Äô, invece di uno dritto '.\n\n\n\n\n\n\n\nNote\n\n\n\nNota: qui sopra sono stati resi visibili gli spazi in eccesso con il carattere ¬∑.\n\n\n\n\n\nRiduzione della distanza con passaggi successivi di normalizzazione\n\n\n\n\n\n\n\n\n\ndescrizione\nleft\nright\ndistanza\nfunzione aggiunta\n\n\n\n\nInizio\nForza¬∑¬∑D‚ÄôAgro¬∑\nForza¬∑d'Agr√≤\n7\n\n\n\nIn minuscolo\nforza¬∑¬∑d‚Äôagro¬∑\nforza¬∑d'agr√≤\n6\nLOWER('value')\n\n\nRimossi spazi ridondanti\nforza¬∑d‚Äôagro\nforza¬∑d'agr√≤\n5\nregexp_replace(trim(LOWER('value')), '\\s+', ' ')\n\n\nRimossi accentati\nforza¬∑d‚Äôagro\nforza¬∑d'agro\n3\nstrip_accents(regexp_replace(trim(LOWER('value')), '\\s+', ' '))\n\n\nRimossi caratteri speciali\nforza¬∑dagro\nforza¬∑dagro\n0\nregexp_replace(strip_accents(regexp_replace(trim(LOWER('value')), '\\s+', ' ')), '[^a-zA-Z0-9 ]', '', 'g')\n\n\n\n\n\n üëâ Quindi un caso molto brutto, pieno di errori, pu√≤ essere normalizzato in modo da ridurre la distanza di Levenshtein a zero, e quindi trovare la corrispondenza esatta e Forza¬∑¬∑D'Agro¬∑ √® uguale a Forza¬∑d'Agr√≤ e quindi riesco a ricavarne il codice identificativo."
  },
  {
    "objectID": "doc_sources/introduction.html#note-finali",
    "href": "doc_sources/introduction.html#note-finali",
    "title": "Perch√© √® stato creato Tom√©to Tomato",
    "section": "Note finali",
    "text": "Note finali\nIn questo percorso abbiamo descritto alcuni elementi di base per misurare la ‚Äúdistanza‚Äù tra le stringhe e a normalizzare il testo per rendere il confronto pi√π efficace. Abbiamo visto come gestire maiuscole/minuscole, spazi superflui, accenti e caratteri speciali, che sono gli elementi di base di ogni processo di pulizia dei dati.\nTuttavia, questo √® solo l‚Äôinizio. Il mondo del ‚Äúfuzzy matching‚Äù √® molto pi√π vasto e complesso. Per affrontare dataset pi√π grandi e ‚Äúdisordinati‚Äù, si ricorre spesso a metodi pi√π sofisticati, che vanno oltre il semplice conteggio delle modifiche:\n\nAlgoritmi fonetici: Invece di guardare come sono scritte le parole, questi algoritmi le codificano in base a come suonano. Metodi come Metaphone o Soundex sono bravissimi a capire che ‚ÄúSmith‚Äù e ‚ÄúSmythe‚Äù sono probabilmente la stessa cosa.\nModelli statistici (n-gram): Questi metodi scompongono le stringhe in piccole parti (es. coppie o triplette di caratteri) e ne confrontano la frequenza, risultando molto efficaci nel trovare somiglianze anche quando l‚Äôordine delle parole √® diverso.\n\nStrumenti di pulizia dati come OpenRefine integrano decine di questi algoritmi avanzati, permettendo di raggruppare e correggere dati simili con grande efficacia."
  },
  {
    "objectID": "doc_sources/introduction.html#footnotes",
    "href": "doc_sources/introduction.html#footnotes",
    "title": "Why Fuzzy Matching?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCodes of Italian municipalities‚Ü©Ô∏é"
  },
  {
    "objectID": "doc_sources/RELEASING.html",
    "href": "doc_sources/RELEASING.html",
    "title": "Releasing and Versioning tometo_tomato",
    "section": "",
    "text": "This document outlines the process for releasing and versioning the tometo_tomato project, leveraging Git tags and setuptools_scm for automatic version management.\n\n\ntometo_tomato uses setuptools_scm to automatically determine its version based on Git tags. This means you do not need to manually update version numbers in pyproject.toml or setup.py. The version is derived directly from your Git history.\nThe version format typically follows Semantic Versioning: MAJOR.MINOR.PATCH.\n\nMAJOR: Incremented for incompatible API changes.\nMINOR: Incremented for adding functionality in a backward-compatible manner.\nPATCH: Incremented for backward-compatible bug fixes.\n\nsetuptools_scm also handles development versions: - X.Y.Z.devN: Indicates a development version, N commits after tag X.Y.Z. - +dirty: Appended if there are uncommitted changes in your working directory.\n\n\n\nFollow these steps to create a new release:\n\nEnsure Your Changes Are Committed: Before creating a release tag, make sure all the changes you want to include in the release are committed to your main branch (or the branch you are releasing from).\ngit status\ngit add .\ngit commit -m \"feat: Prepare for vX.Y.Z release\" # Or appropriate commit message\nCreate a Git Tag: Create an annotated Git tag for the new version. This tag will be used by setuptools_scm to determine the release version.\ngit tag -a vX.Y.Z -m \"Release vX.Y.Z\"\nReplace X.Y.Z with the actual version number (e.g., v1.0.0).\nPush the Tag to GitHub: Push the newly created tag to your GitHub repository.\ngit push origin vX.Y.Z\nYou might also want to push all commits if you haven‚Äôt already:\ngit push origin main\nGitHub Actions (Optional but Recommended): If your repository is configured with GitHub Actions for releases (e.g., a workflow that triggers on new tags), pushing the tag will automatically trigger the build, testing, and release process (e.g., publishing to PyPI, creating a GitHub Release).\nVerify the Version: After the release process is complete (and if you‚Äôve installed the new version), you can verify the version of the tometo_tomato tool:\npython3 src/tometo_tomato.py --version\nThis should output the exact version you tagged (e.g., tometo_tomato.py vX.Y.Z).\n\n\n\n\nDuring development, setuptools_scm will automatically generate a version string that reflects the current state of your repository. For example, if your last tag was v0.1.0 and you have made 5 commits since then, your development version might look like 0.1.0.dev5. If you have uncommitted changes, it will append +dirty (e.g., 0.1.0.dev5+dirty).\nThis provides immediate feedback on the exact code state you are working with, which is invaluable for debugging and collaboration."
  },
  {
    "objectID": "doc_sources/RELEASING.html#versioning-strategy",
    "href": "doc_sources/RELEASING.html#versioning-strategy",
    "title": "Releasing and Versioning tometo_tomato",
    "section": "",
    "text": "tometo_tomato uses setuptools_scm to automatically determine its version based on Git tags. This means you do not need to manually update version numbers in pyproject.toml or setup.py. The version is derived directly from your Git history.\nThe version format typically follows Semantic Versioning: MAJOR.MINOR.PATCH.\n\nMAJOR: Incremented for incompatible API changes.\nMINOR: Incremented for adding functionality in a backward-compatible manner.\nPATCH: Incremented for backward-compatible bug fixes.\n\nsetuptools_scm also handles development versions: - X.Y.Z.devN: Indicates a development version, N commits after tag X.Y.Z. - +dirty: Appended if there are uncommitted changes in your working directory."
  },
  {
    "objectID": "doc_sources/RELEASING.html#release-process",
    "href": "doc_sources/RELEASING.html#release-process",
    "title": "Releasing and Versioning tometo_tomato",
    "section": "",
    "text": "Follow these steps to create a new release:\n\nEnsure Your Changes Are Committed: Before creating a release tag, make sure all the changes you want to include in the release are committed to your main branch (or the branch you are releasing from).\ngit status\ngit add .\ngit commit -m \"feat: Prepare for vX.Y.Z release\" # Or appropriate commit message\nCreate a Git Tag: Create an annotated Git tag for the new version. This tag will be used by setuptools_scm to determine the release version.\ngit tag -a vX.Y.Z -m \"Release vX.Y.Z\"\nReplace X.Y.Z with the actual version number (e.g., v1.0.0).\nPush the Tag to GitHub: Push the newly created tag to your GitHub repository.\ngit push origin vX.Y.Z\nYou might also want to push all commits if you haven‚Äôt already:\ngit push origin main\nGitHub Actions (Optional but Recommended): If your repository is configured with GitHub Actions for releases (e.g., a workflow that triggers on new tags), pushing the tag will automatically trigger the build, testing, and release process (e.g., publishing to PyPI, creating a GitHub Release).\nVerify the Version: After the release process is complete (and if you‚Äôve installed the new version), you can verify the version of the tometo_tomato tool:\npython3 src/tometo_tomato.py --version\nThis should output the exact version you tagged (e.g., tometo_tomato.py vX.Y.Z)."
  },
  {
    "objectID": "doc_sources/RELEASING.html#development-workflow",
    "href": "doc_sources/RELEASING.html#development-workflow",
    "title": "Releasing and Versioning tometo_tomato",
    "section": "",
    "text": "During development, setuptools_scm will automatically generate a version string that reflects the current state of your repository. For example, if your last tag was v0.1.0 and you have made 5 commits since then, your development version might look like 0.1.0.dev5. If you have uncommitted changes, it will append +dirty (e.g., 0.1.0.dev5+dirty).\nThis provides immediate feedback on the exact code state you are working with, which is invaluable for debugging and collaboration."
  },
  {
    "objectID": "doc_sources/project_analysis.html",
    "href": "doc_sources/project_analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This document summarizes the analysis of the tometo_tomato.py project, highlighting its strengths, weaknesses, and potential areas for improvement.\n\n\nThe tometo_tomato.py project is a well-structured and functional solution to a practical problem. It effectively uses modern data tools (DuckDB, rapidfuzz) and provides a user-friendly CLI. The recent improvements (output logic, uniqueness, handling spaces in field names) have made it more robust. The main challenge seems to be the rapidfuzz extension installation/loading, which is an external dependency issue.\nIt‚Äôs a solid foundation that can be further enhanced with more robust error handling, comprehensive testing, and potentially more advanced CSV parsing.\n\n\n\n\nClear Purpose: The project clearly addresses the problem of fuzzy joining tabular data, which is a common real-world challenge.\nLeverages Powerful Tools: Uses DuckDB for efficient data processing and rapidfuzz (or its fallbacks) for robust fuzzy matching.\nModular Design: The code is broken down into functions (parse_args, read_header, build_join_pairs, prepare_select_clauses, try_load_rapidfuzz, choose_score_expr, main), which improves readability and maintainability.\nCLI Interface: Provides a command-line interface, making it easy to use and integrate into workflows.\nError Handling (Fuzzy Functions): Includes logic to try and install rapidfuzz and fall back to built-in Levenshtein/Damerau-Levenshtein functions if rapidfuzz is unavailable.\nOutput Control: Allows specifying output files for clean and ambiguous matches, and now conditionally generates the ambiguous output.\nUniqueness Handling: Implements logic to ensure unique output records based on join fields.\nHandles Field Names with Spaces: The recent fix ensures it can handle column names with spaces.\nClear Comments: The code has good comments explaining the purpose of functions and complex logic.\n\n\n\n\n\nrapidfuzz Installation/Loading Robustness:\n\nThe HTTP 403 error when installing rapidfuzz from within the Python script is a recurring issue. While a manual workaround exists, the script‚Äôs try_load_rapidfuzz could be more robust. It might be beneficial to:\n\nProvide clearer instructions on how to pre-install extensions if the automatic method fails.\nConsider using duckdb.install_extension() and duckdb.load_extension() directly, and perhaps catching more specific exceptions.\nIf the HTTP 403 is a persistent issue, consider bundling the extension or providing an alternative download mechanism.\n\n\nError Handling (General):\n\nThe script exits with sys.exit(1) on some errors (e.g., no join pair found, no fuzzy function available). While functional, a more structured error handling (e.g., custom exceptions) could make the script more robust for integration into larger systems.\n\nInput/Reference File Handling:\n\nThe read_header function is a bit simplistic (naive split on comma). While it works for simple CSVs, it might break for CSVs with commas within quoted fields. Using Python‚Äôs csv module for reading headers would be more robust.\nThe script assumes header=true and all_varchar=true for read_csv_auto. While common, making these configurable might add flexibility.\n\nmain Function Complexity:\n\nThe main function is quite long and performs many different tasks (argument parsing, join pair building, select clause preparation, DuckDB connection, SQL execution, file post-processing, printing messages). Breaking it down into smaller, more focused functions could improve readability and maintainability. For example, a separate function for ‚Äúexecute_fuzzy_join_sql‚Äù or ‚Äúhandle_output_files‚Äù.\n\nTestability:\n\nThe script currently lacks unit tests. Adding unit tests for functions like build_join_pairs, prepare_select_clauses, and choose_score_expr would significantly improve code quality and prevent regressions.\n\nCLI Argument Defaults:\n\nThe default=None for --output-ambiguous is good. Consider if other arguments could benefit from more explicit defaults or validation.\n\nDocstrings and Type Hinting:\n\nThe functions have type hints, which is excellent. Ensuring comprehensive docstrings for all functions would further improve clarity.\n\n\n\n\n\n\nStrengths ‚Ä¢ Clear problem focus ‚Äì fuzzy CSV joins ‚Äì with a pragmatic tech-stack (DuckDB + rapidfuzz). ‚Ä¢ Good documentation: README, PRD and extra markdown files give context, usage examples, and Italian + English coverage. ‚Ä¢ Tests already present (tests/test_core.py) and CI workflow is configured. ‚Ä¢ Single-file Python implementation keeps the learning curve low and is easy to inspect. ‚Ä¢ Cross-platform: pure-Python, no compiled extensions required.\nAreas for improvement & concrete suggestions\n3 Code organisation ‚Äì Split the 400-line tometo_tomato.py into logical modules: ‚Ä¢ cli.py for argparse / typer ‚Ä¢ engine.py for DuckDB logic ‚Ä¢ io_utils.py for CSV/header helpers This boosts readability and unit-test granularity. ‚Äì Add type hints throughout and run mypy in CI. 4 Performance & memory ‚Äì For very large CSVs you materialise full cross-joins in DuckDB; memory can explode. Investigate DUCKDB_DISABLE_OBJECT_CACHE, chunked processing, or blocking with LIMIT. ‚Äì Consider writing scores into a temp table with ORDER BY/LIMIT 1 instead of ROW_NUMBER() if memory becomes a problem. 5 Robustness ‚Äì At runtime, call sql_safe_identifier or double-quote column names produced from user input to prevent SQL-injection or invalid identifiers (edge-cases with spaces, quotes, UTF-8). ‚Äì Validate that ‚Äìoutput-clean and ‚Äìoutput-ambiguous don‚Äôt overwrite one of the input files. 6 Logging & UX\n‚Äì Emit a JSON or TSV summary line for easy downstream parsing (records processed, matches, ambiguous count, elapsed time).\n7 Testing ‚Äì Expand tests to cover: ‚Ä¢ ‚Äìinfer-pairs with threshold edge-cases. ‚Ä¢ Large file sampling (property-based tests with hypothesis). ‚Ä¢ Failure paths (missing columns, unreadable file). ‚Äì Add linting (ruff, black) and static-type checks to CI. 8 Documentation ‚Äì Move command-line examples into docs/usage.md and wire up mkdocs or sphinx for a browsable site. ‚Äì Provide a benchmark section comparing execution time with and without rapidfuzz.\nNext steps If you‚Äôd like to apply any of these suggestions, let me know which direction you prefer. I will then tell you exactly which files are most likely to need edits so you can add them to the chat."
  },
  {
    "objectID": "doc_sources/project_analysis.html#project-analysis-tometo_tomato.py",
    "href": "doc_sources/project_analysis.html#project-analysis-tometo_tomato.py",
    "title": "Analysis",
    "section": "",
    "text": "This document summarizes the analysis of the tometo_tomato.py project, highlighting its strengths, weaknesses, and potential areas for improvement.\n\n\nThe tometo_tomato.py project is a well-structured and functional solution to a practical problem. It effectively uses modern data tools (DuckDB, rapidfuzz) and provides a user-friendly CLI. The recent improvements (output logic, uniqueness, handling spaces in field names) have made it more robust. The main challenge seems to be the rapidfuzz extension installation/loading, which is an external dependency issue.\nIt‚Äôs a solid foundation that can be further enhanced with more robust error handling, comprehensive testing, and potentially more advanced CSV parsing.\n\n\n\n\nClear Purpose: The project clearly addresses the problem of fuzzy joining tabular data, which is a common real-world challenge.\nLeverages Powerful Tools: Uses DuckDB for efficient data processing and rapidfuzz (or its fallbacks) for robust fuzzy matching.\nModular Design: The code is broken down into functions (parse_args, read_header, build_join_pairs, prepare_select_clauses, try_load_rapidfuzz, choose_score_expr, main), which improves readability and maintainability.\nCLI Interface: Provides a command-line interface, making it easy to use and integrate into workflows.\nError Handling (Fuzzy Functions): Includes logic to try and install rapidfuzz and fall back to built-in Levenshtein/Damerau-Levenshtein functions if rapidfuzz is unavailable.\nOutput Control: Allows specifying output files for clean and ambiguous matches, and now conditionally generates the ambiguous output.\nUniqueness Handling: Implements logic to ensure unique output records based on join fields.\nHandles Field Names with Spaces: The recent fix ensures it can handle column names with spaces.\nClear Comments: The code has good comments explaining the purpose of functions and complex logic.\n\n\n\n\n\nrapidfuzz Installation/Loading Robustness:\n\nThe HTTP 403 error when installing rapidfuzz from within the Python script is a recurring issue. While a manual workaround exists, the script‚Äôs try_load_rapidfuzz could be more robust. It might be beneficial to:\n\nProvide clearer instructions on how to pre-install extensions if the automatic method fails.\nConsider using duckdb.install_extension() and duckdb.load_extension() directly, and perhaps catching more specific exceptions.\nIf the HTTP 403 is a persistent issue, consider bundling the extension or providing an alternative download mechanism.\n\n\nError Handling (General):\n\nThe script exits with sys.exit(1) on some errors (e.g., no join pair found, no fuzzy function available). While functional, a more structured error handling (e.g., custom exceptions) could make the script more robust for integration into larger systems.\n\nInput/Reference File Handling:\n\nThe read_header function is a bit simplistic (naive split on comma). While it works for simple CSVs, it might break for CSVs with commas within quoted fields. Using Python‚Äôs csv module for reading headers would be more robust.\nThe script assumes header=true and all_varchar=true for read_csv_auto. While common, making these configurable might add flexibility.\n\nmain Function Complexity:\n\nThe main function is quite long and performs many different tasks (argument parsing, join pair building, select clause preparation, DuckDB connection, SQL execution, file post-processing, printing messages). Breaking it down into smaller, more focused functions could improve readability and maintainability. For example, a separate function for ‚Äúexecute_fuzzy_join_sql‚Äù or ‚Äúhandle_output_files‚Äù.\n\nTestability:\n\nThe script currently lacks unit tests. Adding unit tests for functions like build_join_pairs, prepare_select_clauses, and choose_score_expr would significantly improve code quality and prevent regressions.\n\nCLI Argument Defaults:\n\nThe default=None for --output-ambiguous is good. Consider if other arguments could benefit from more explicit defaults or validation.\n\nDocstrings and Type Hinting:\n\nThe functions have type hints, which is excellent. Ensuring comprehensive docstrings for all functions would further improve clarity."
  },
  {
    "objectID": "doc_sources/project_analysis.html#o3",
    "href": "doc_sources/project_analysis.html#o3",
    "title": "Analysis",
    "section": "",
    "text": "Strengths ‚Ä¢ Clear problem focus ‚Äì fuzzy CSV joins ‚Äì with a pragmatic tech-stack (DuckDB + rapidfuzz). ‚Ä¢ Good documentation: README, PRD and extra markdown files give context, usage examples, and Italian + English coverage. ‚Ä¢ Tests already present (tests/test_core.py) and CI workflow is configured. ‚Ä¢ Single-file Python implementation keeps the learning curve low and is easy to inspect. ‚Ä¢ Cross-platform: pure-Python, no compiled extensions required.\nAreas for improvement & concrete suggestions\n3 Code organisation ‚Äì Split the 400-line tometo_tomato.py into logical modules: ‚Ä¢ cli.py for argparse / typer ‚Ä¢ engine.py for DuckDB logic ‚Ä¢ io_utils.py for CSV/header helpers This boosts readability and unit-test granularity. ‚Äì Add type hints throughout and run mypy in CI. 4 Performance & memory ‚Äì For very large CSVs you materialise full cross-joins in DuckDB; memory can explode. Investigate DUCKDB_DISABLE_OBJECT_CACHE, chunked processing, or blocking with LIMIT. ‚Äì Consider writing scores into a temp table with ORDER BY/LIMIT 1 instead of ROW_NUMBER() if memory becomes a problem. 5 Robustness ‚Äì At runtime, call sql_safe_identifier or double-quote column names produced from user input to prevent SQL-injection or invalid identifiers (edge-cases with spaces, quotes, UTF-8). ‚Äì Validate that ‚Äìoutput-clean and ‚Äìoutput-ambiguous don‚Äôt overwrite one of the input files. 6 Logging & UX\n‚Äì Emit a JSON or TSV summary line for easy downstream parsing (records processed, matches, ambiguous count, elapsed time).\n7 Testing ‚Äì Expand tests to cover: ‚Ä¢ ‚Äìinfer-pairs with threshold edge-cases. ‚Ä¢ Large file sampling (property-based tests with hypothesis). ‚Ä¢ Failure paths (missing columns, unreadable file). ‚Äì Add linting (ruff, black) and static-type checks to CI. 8 Documentation ‚Äì Move command-line examples into docs/usage.md and wire up mkdocs or sphinx for a browsable site. ‚Äì Provide a benchmark section comparing execution time with and without rapidfuzz.\nNext steps If you‚Äôd like to apply any of these suggestions, let me know which direction you prefer. I will then tell you exactly which files are most likely to need edits so you can add them to the chat."
  },
  {
    "objectID": "index.html#how-it-works",
    "href": "index.html#how-it-works",
    "title": "Tometo Tomato",
    "section": "How It Works",
    "text": "How It Works\nThe workflow has two steps:\n Your file           Reference file\n (dirty data)        (ground truth)\n      ‚îÇ                     ‚îÇ\n      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ tometo_tomato ‚îÄ‚îÄ‚îò\n                 ‚îÇ\n           Mapping table\n           (dirty ‚Üí clean + score)\n                 ‚îÇ\n      ‚îå‚îÄ‚îÄ exact join (duckdb) ‚îÄ‚îÄ‚îê\n      ‚îÇ                         ‚îÇ\n  Your file               Corrected file\n  (original)              (enriched)\nStep 1. Run tometo_tomato to fuzzy-match your dirty column against a reference file. The output is a mapping table that links each dirty value to the best match.\nStep 2. Use a standard exact join (e.g.¬†with DuckDB) to bring the corrections back into your original file."
  },
  {
    "objectID": "index.html#quick-example",
    "href": "index.html#quick-example",
    "title": "Tometo Tomato",
    "section": "Quick Example",
    "text": "Quick Example\n\n\n\nYour working file\n\n\nmunicipality\nregion\n\n\n\n\nCastro\nPugla\n\n\nCastro\nLombardia\n\n\nCalliano\nTrentno-Alto Adige\n\n\nCalliano\nPiemnte\n\n\n\n\n\n\n\nReference file\n\n\nmunicipality_name\nregion\nistat_code\n\n\n\n\nCastro\nPuglia\n075019\n\n\nCastro\nLombardia\n016065\n\n\nCalliano\nTrentino-Alto Adige\n022032\n\n\nCalliano\nPiemonte\n005013\n\n\n\n\n\ntometo_tomato work.csv reference.csv \\\n  -j \"municipality,municipality_name\" \\\n  -j \"region,region\" \\\n  -a istat_code -s -t 70 \\\n  -o mapping.csv\n\nMapping result ‚Äî each dirty value linked to its best match\n\n\n\n\n\n\n\n\n\n\nmunicipality\nregion\nref_municipality_name\nref_region\nistat_code\navg_score\n\n\n\n\nCastro\nPugla\nCastro\nPuglia\n075019\n95.5\n\n\nCastro\nLombardia\nCastro\nLombardia\n016065\n100.0\n\n\nCalliano\nTrentno-Alto Adige\nCalliano\nTrentino-Alto Adige\n022032\n98.6\n\n\nCalliano\nPiemnte\nCalliano\nPiemonte\n005013\n96.7\n\n\n\nMatching on two column pairs (-j repeated) ensures ‚ÄúCastro, Pugla‚Äù maps to the correct Castro in Puglia, not the one in Lombardia.\n\nWhy fuzzy matching? Install Use Case Guide CLI Reference"
  },
  {
    "objectID": "doc_sources/introduction.html#the-problem",
    "href": "doc_sources/introduction.html#the-problem",
    "title": "Why Fuzzy Matching?",
    "section": "",
    "text": "Anyone who works with data regularly encounters columns that should follow a standard encoding ‚Äî an official list of values ‚Äî but instead contain typos, extra whitespace, special characters instead of accented letters, inconsistent capitalization, and so on.\nHere are some Italian city names, written incorrectly:\n\nExamples of incorrect city names\n\n\n\n\n\n\ncity\ntype of error\n\n\n\n\nCefalu‚Äô\nshould use √π instead of u'\n\n\nReggio Calabria\nthe official name is Reggio di Calabria\n\n\nRODENGO-SAIANO\nshould be Rodengo Saiano, without - and not all uppercase\n\n\n\n If you wanted to associate each city with the code1 that ISTAT ‚Äî the Italian National Institute of Statistics ‚Äî assigns to each municipality, a simple exact join would fail because the names don‚Äôt match. Associating a code to each municipality is very important: it enables you to link data from different sources that would otherwise be incomparable, and to automatically generate maps, since mapping software often uses these codes to identify municipalities.\n\n\n\n\n\n\nPay Attention\n\n\n\nData educators usually shout: CODES, NOT LABELS!!\n\n\nThe codes for Italian administrative units are public and freely available under CC-BY-4.0 on various sections of the ISTAT website. One source is SITUAS, which has a page with the ‚ÄúList of codes and names of territorial units‚Äù, downloadable in CSV and JSON format.\n\nOfficial ISTAT municipality data\n\n\ncity\nregion\nistat_city_code\n\n\n\n\nCefal√π\nSicilia\n082027\n\n\nReggio di Calabria\nCalabria\n080063\n\n\nRodengo Saiano\nLombardia\n017163\n\n\n\n If you try to join the incorrect data with this official data, you get no results.\n\nTometo Tomato was created to solve this problem: it lets you easily perform joins that are not based on exact matches, but on similarity, and then enrich, modify, and correct your messy input table by comparing it against a reference table.\n\n\n\nThese are our two sample tables, available as input.csv and ref_sample.csv so you can download them and try the examples yourself."
  },
  {
    "objectID": "doc_sources/introduction.html#using-sql",
    "href": "doc_sources/introduction.html#using-sql",
    "title": "Why Fuzzy Matching?",
    "section": "Using SQL",
    "text": "Using SQL\n\nThe Exact JOIN\nThe first test is to run a simple SQL join query to see what happens with our sample data.\n\n\n\nThe raw data\n\n\ncity\nregion\n\n\n\n\nCefalu‚Äô\nSicilia\n\n\nReggio Calabria\nCALABRIA\n\n\nRODENGO-SAIANO\nLombardia\n\n\n\n\n\n\n\nThe reference data\n\n\ncity\nregion\ncity_code\n\n\n\n\nCefal√π\nSicilia\n082027\n\n\nReggio di Calabria\nCalabria\n080063\n\n\nRodengo Saiano\nLombardia\n017164\n\n\n\n\n\n The query below uses a LEFT JOIN so that all rows from the left table appear in the output, even if they have no match in the right table.\nSELECT\n  i.*,\n  r.city_code\nFROM read_csv_auto('input.csv') AS i\nLEFT JOIN read_csv_auto('ref_sample.csv') AS r\n  ON i.city = r.city AND i.region = r.region\n\n\n\n\n\n\nNote\n\n\n\nNote: read_csv_auto is a DuckDB function that reads CSV files directly without importing them into a table first. This makes quick experiments easy ‚Äî no temporary tables needed.\n\n\nThe result is disappointing: none of the rows find a match, so city_code is always NULL.\n\nResult of an exact join between messy and official data\n\n\ncity\nregion\nistat_city_code\n\n\n\n\nCefalu‚Äô\nSicilia\nNULL\n\n\nReggio Calabria\nCALABRIA\nNULL\n\n\nRODENGO-SAIANO\nLombardia\nNULL\n\n\n\n\n\nThe Fuzzy JOIN\nA ‚Äúfuzzy‚Äù join finds matches even when values are not exactly equal, but merely similar. For example, we could consider city names that differ by just a few characters, using the Levenshtein distance (the minimum number of single-character edits needed to transform one string into another).\nWe could rewrite the previous query using DuckDB‚Äôs levenshtein function, matching cities that differ by at most 2 characters:\nSELECT\n  i.city AS input_city,\n  i.region AS input_region,\n  r.city AS ref_city,\n  r.region AS ref_region,\n  r.city_code,\n  levenshtein(i.city, r.city) AS levenshtein_distance\nFROM read_csv_auto('input.csv') AS i\nJOIN read_csv_auto('ref_sample.csv') AS r\n  ON i.region = r.region\n  AND levenshtein(i.city, r.city) &lt;= 2;\n\nResult with Levenshtein distance ‚â§ 2\n\n\n\n\n\n\n\n\n\n\ninput_city\ninput_region\nref_city\nref_region\ncity_code\nlevenshtein_distance\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n2\n\n\n\nThe only match is Cefalu', because the Levenshtein distance between Cefalu' and Cefal√π is 2: replacing u' with √π and removing '.\nIf we increase the threshold to 10, nothing changes ‚Äî because Rodengo Saiano is written in uppercase with a hyphen, and the distance between RODENGO-SAIANO and Rodengo Saiano is 12:\nSELECT levenshtein('RODENGO-SAIANO', 'Rodengo Saiano') AS distance;\n\n12\nSetting the threshold to 15 gives only 2 matches, because the exact JOIN on region fails between CALABRIA and Calabria. So we need to use a distance function for the region column too:\nSELECT\n  i.city AS input_city,\n  i.region AS input_region,\n  r.city AS ref_city,\n  r.region AS ref_region,\n  r.city_code,\n  levenshtein(i.city, r.city) AS levenshtein_distance\nFROM read_csv_auto('input.csv') AS i\nJOIN read_csv_auto('ref_sample.csv') AS r\n  ON levenshtein(i.city, r.city) &lt;= 15\n  AND levenshtein(LOWER(i.region), LOWER(r.region)) &lt; 10\nBut the output is not what we expect ‚Äî not 3 rows (one per municipality), but 8 rows:\n\nResult of a fuzzy join with permissive thresholds\n\n\n\n\n\n\n\n\n\n\ninput_city\ninput_region\nref_city\nref_region\ncity_code\nlevenshtein_distance\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n2\n\n\nCefalu‚Äô\nSicilia\nReggio di Calabria\nCalabria\n080063\n15\n\n\nCefalu‚Äô\nSicilia\nRodengo Saiano\nLombardia\n017163\n12\n\n\nReggio Calabria\nCALABRIA\nCefal√π\nSicilia\n082027\n12\n\n\nReggio Calabria\nCALABRIA\nReggio di Calabria\nCalabria\n080063\n3\n\n\nReggio Calabria\nCALABRIA\nRodengo Saiano\nLombardia\n017163\n10\n\n\nRODENGO-SAIANO\nLombardia\nCefal√π\nSicilia\n082027\n14\n\n\nRODENGO-SAIANO\nLombardia\nRodengo Saiano\nLombardia\n017163\n12\n\n\n\nWith exact joins, the goal is to find a single clear match for each row. In the world of fuzzy matching, the rules change. By using permissive distance thresholds, we are no longer asking the database ‚Äúfind the one correct match‚Äù, but rather:\n\nFor each input row, find all reference rows that satisfy these loose similarity criteria.\n\nIf an input row is ‚Äúsomewhat similar‚Äù to multiple reference rows, the database creates an output row for each. This multiplication effect is known as a Cartesian product of matches.\nAfter generating all possible matches, we need to filter them to keep only the best one per input record. The process has three phases:\n\nFind all possible matches and calculate distances\nRank each match by total distance\nSelect only the top-ranked match (the most similar)\n\n-- Phase 1: find all candidates and calculate distances\nWITH all_matches AS (\n    SELECT\n        i.city AS input_city,\n        i.region AS input_region,\n        r.city AS ref_city,\n        r.region AS ref_region,\n        r.city_code,\n        levenshtein(i.city, r.city) AS city_distance,\n        levenshtein(i.region, r.region) AS region_distance\n    FROM read_csv_auto('input.csv') AS i\n    JOIN read_csv_auto('ref_sample.csv') AS r\n      ON levenshtein(i.city, r.city) &lt;= 15\n     AND levenshtein(i.region, r.region) &lt; 10\n),\n-- Phase 2: rank the candidates\nranked_matches AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (\n            PARTITION BY input_city, input_region\n            ORDER BY (city_distance + region_distance) ASC\n        ) AS rn\n    FROM all_matches\n)\n-- Phase 3: keep only the best candidate (rn = 1)\nSELECT\n    input_city,\n    input_region,\n    ref_city,\n    ref_region,\n    city_code,\n    city_distance,\n    region_distance\nFROM ranked_matches\nWHERE rn = 1;\n\nResult of a fuzzy join, selecting the best match\n\n\n\n\n\n\n\n\n\n\n\ninput_city\ninput_region\nref_city\nref_region\ncity_code\ncity_distance\nregion_distance\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n2\n0\n\n\nRODENGO-SAIANO\nLombardia\nRodengo Saiano\nLombardia\n017163\n12\n0\n\n\nReggio Calabria\nCALABRIA\nReggio di Calabria\nCalabria\n080063\n3\n7\n\n\n\n\n\nReducing Distances with Normalization\nHere is a summary using the Italian municipality of Forza d'Agro, written incorrectly as Forza  D'Agro:\n\nuppercase D instead of lowercase\ntwo extra spaces (one between Forza and D'Agro, one trailing)\nno accent on the o\na typographic apostrophe \\u2019 instead of a straight one '\n\n\n\n\n\n\n\nNote\n\n\n\nNote: spaces are shown as ¬∑ for visibility.\n\n\n\n\n\nReducing Levenshtein distance through normalization steps\n\n\n\n\n\n\n\n\n\ndescription\nleft\nright\ndistance\nfunction applied\n\n\n\n\nStart\nForza¬∑¬∑D\\u2019Agro¬∑\nForza¬∑d'Agr\\u00f2\n7\n\n\n\nLowercase\nforza¬∑¬∑d\\u2019agro¬∑\nforza¬∑d'agr\\u00f2\n6\nLOWER('value')\n\n\nTrim + collapse spaces\nforza¬∑d\\u2019agro\nforza¬∑d'agr\\u00f2\n5\nregexp_replace(trim(LOWER('value')), '\\s+', ' ')\n\n\nStrip accents\nforza¬∑d\\u2019agro\nforza¬∑d'agro\n3\nstrip_accents(...)\n\n\nKeep only alphanumeric\nforza¬∑dagro\nforza¬∑dagro\n0\nregexp_replace(..., '[^a-zA-Z0-9 ]', '', 'g')\n\n\n\n\n\n\nSo even a really messy value, full of errors, can be normalized to reduce the distance to zero ‚Äî meaning Forza  D\\u2019Agro becomes equivalent to Forza d'Agr\\u00f2 and we can retrieve its identifier code."
  },
  {
    "objectID": "doc_sources/introduction.html#what-comes-next",
    "href": "doc_sources/introduction.html#what-comes-next",
    "title": "Why Fuzzy Matching?",
    "section": "What Comes Next",
    "text": "What Comes Next\nIn this walkthrough we covered the basics of measuring string distance and normalizing text for more effective comparison: case, whitespace, accents, and special characters.\nHowever, this is just the beginning. The fuzzy matching world is much broader. For larger and messier datasets, more sophisticated methods exist:\n\nPhonetic algorithms: Instead of looking at how words are spelled, these encode words by how they sound. Methods like Metaphone or Soundex excel at recognizing that ‚ÄúSmith‚Äù and ‚ÄúSmythe‚Äù are probably the same thing.\nStatistical models (n-grams): These break strings into small fragments (pairs or triplets of characters) and compare their frequency, which is very effective at finding similarities even when word order differs.\n\nData cleaning tools like OpenRefine integrate dozens of these advanced algorithms, allowing you to cluster and correct similar data with great effectiveness.\nTometo Tomato wraps all of this complexity into a single command. See the Use Case Guide for the complete step-by-step workflow."
  },
  {
    "objectID": "doc_sources/cli-reference.html",
    "href": "doc_sources/cli-reference.html",
    "title": "CLI Reference",
    "section": "",
    "text": "tometo_tomato INPUT_FILE REFERENCE_FILE [OPTIONS]\nINPUT_FILE is the CSV with messy data. REFERENCE_FILE is the CSV with correct, authoritative data."
  },
  {
    "objectID": "doc_sources/cli-reference.html#synopsis",
    "href": "doc_sources/cli-reference.html#synopsis",
    "title": "CLI Reference",
    "section": "",
    "text": "tometo_tomato INPUT_FILE REFERENCE_FILE [OPTIONS]\nINPUT_FILE is the CSV with messy data. REFERENCE_FILE is the CSV with correct, authoritative data."
  },
  {
    "objectID": "doc_sources/cli-reference.html#options",
    "href": "doc_sources/cli-reference.html#options",
    "title": "CLI Reference",
    "section": "Options",
    "text": "Options\n\nCore\n\nCore options\n\n\n\n\n\n\n\nFlag\nShort\nDescription\n\n\n\n\n--join-pair COL1,COL2\n-j\nColumn pair to compare (input column, reference column). Repeatable for multi-column matching.\n\n\n--add-field FIELD\n-a\nExtra column from the reference file to include in output. Repeatable.\n\n\n--output-clean FILE\n-o\nPath for the clean matches output file. Default: clean_matches.csv\n\n\n--output-ambiguous FILE\n-u\nPath for the ambiguous matches file. Only created if ambiguous records exist.\n\n\n--threshold N\n-t\nMinimum similarity score (0-100). Default: 85\n\n\n--show-score\n-s\nInclude the avg_score column in the output.\n\n\n--force\n-f\nOverwrite existing output files without prompting.\n\n\n\n\n\nMatching Algorithm\n\nMatching options\n\n\n\n\n\n\n\nFlag\nShort\nDescription\n\n\n\n\n--scorer ALGO\n\nFuzzy matching algorithm: ratio (default) or token_set_ratio.\n\n\n--infer-pairs\n-i\nAutomatically infer column pairs from similar header names.\n\n\n--infer-threshold N\n-I\nSimilarity threshold (0-1) for header name inference. Default: 0.7\n\n\n\n\n\nNormalization\nBy default, tometo_tomato normalizes join columns before comparison: converts to lowercase, trims whitespace, and collapses multiple spaces. These flags control that behavior.\n\nNormalization options\n\n\n\n\n\n\n\nFlag\nShort\nDescription\n\n\n\n\n--latinize\n\nStrip accents and special characters before matching (e.g.¬†e = e). Original characters are preserved in output.\n\n\n--keep-alphanumeric\n-k\nRemove punctuation and special characters, keeping only letters, numbers, and spaces.\n\n\n--raw-case\n\nDisable case normalization (case-sensitive matching).\n\n\n--raw-whitespace\n\nDisable whitespace normalization (no trimming or space collapsing).\n\n\n\n\n\nPerformance\n\nPerformance options\n\n\n\n\n\n\n\nFlag\nShort\nDescription\n\n\n\n\n--block-prefix N\n\nOnly compare records sharing the same first N characters in each join column. Dramatically reduces computation on large datasets.\n\n\n\n\n\nOutput Control\n\nOutput control options\n\n\nFlag\nShort\nDescription\n\n\n\n\n--verbose\n-v\nIncrease verbosity. Use -vv for debug output.\n\n\n--quiet\n-q\nSuppress all output except errors.\n\n\n--version\n\nShow version and exit."
  },
  {
    "objectID": "doc_sources/cli-reference.html#examples",
    "href": "doc_sources/cli-reference.html#examples",
    "title": "CLI Reference",
    "section": "Examples",
    "text": "Examples\n\nBasic single-column match\ntometo_tomato work.csv reference.csv \\\n  -j \"city,city_name\" \\\n  -a city_code \\\n  -s -t 85 \\\n  -o mapping.csv\n\n\nMulti-column match (disambiguation)\ntometo_tomato work.csv reference.csv \\\n  -j \"municipality,municipality_name\" \\\n  -j \"region,region\" \\\n  -a istat_code \\\n  -s -t 70 \\\n  -o mapping.csv\n\n\nWith normalization\ntometo_tomato work.csv reference.csv \\\n  -j \"name,ref_name\" \\\n  --latinize \\\n  --keep-alphanumeric \\\n  -a code \\\n  -s -t 80 \\\n  -o mapping.csv\n\n\nWith blocking for large datasets\ntometo_tomato work.csv reference.csv \\\n  -j \"city,city\" -j \"region,region\" \\\n  -a city_code \\\n  --block-prefix 3 \\\n  -s -t 85 \\\n  -o mapping.csv\n\n\nToken set ratio (word-order independent)\nUseful when names have different word counts (e.g.¬†‚ÄúReggio Calabria‚Äù vs.¬†‚ÄúReggio di Calabria‚Äù):\ntometo_tomato work.csv reference.csv \\\n  -j \"city,city_name\" \\\n  --scorer token_set_ratio \\\n  -s -t 90 \\\n  -o mapping.csv\n\n\nAutomated pipeline (no prompts)\ntometo_tomato work.csv reference.csv \\\n  -j \"name,ref_name\" \\\n  -a code \\\n  -t 85 \\\n  -o mapping.csv \\\n  --force --quiet\n\n\nCapture ambiguous matches\ntometo_tomato work.csv reference.csv \\\n  -j \"city,city_name\" \\\n  -a city_code \\\n  -s -t 80 \\\n  -o mapping.csv \\\n  -u ambiguous.csv"
  },
  {
    "objectID": "doc_sources/cli-reference.html#how-scores-work",
    "href": "doc_sources/cli-reference.html#how-scores-work",
    "title": "CLI Reference",
    "section": "How Scores Work",
    "text": "How Scores Work\nThe avg_score column represents the average fuzzy similarity across all join pairs, on a 0-100 scale:\n\n100 = perfect match (after normalization)\n90-99 = minor differences (a missing character, a typo)\n80-89 = moderate differences (abbreviations, missing words)\n&lt; 80 = distant matches (manual review recommended)\n\nWhen using multiple -j pairs, the score is the average of individual pair scores. For example, with -j \"city,city\" -j \"region,region\":\n\ncity score: 100 (exact match)\nregion score: 90.9 (‚ÄúPugla‚Äù vs ‚ÄúPuglia‚Äù)\navg_score: (100 + 90.9) / 2 = 95.5"
  },
  {
    "objectID": "doc_sources/cli-reference.html#how-ambiguity-works",
    "href": "doc_sources/cli-reference.html#how-ambiguity-works",
    "title": "CLI Reference",
    "section": "How Ambiguity Works",
    "text": "How Ambiguity Works\nA match is ambiguous when two or more reference rows achieve the same maximum avg_score for a given input row (and that score meets the threshold).\n\nAmbiguous rows are excluded from --output-clean to prevent inserting incorrect data\nUse --output-ambiguous to inspect the tied candidates\nIf --output-ambiguous is not set, the tool prints a warning with the count of ambiguous records"
  },
  {
    "objectID": "doc_sources/install.html#prerequisites",
    "href": "doc_sources/install.html#prerequisites",
    "title": "Installation",
    "section": "",
    "text": "Make sure you have Python (version 3.8 or higher) installed.\nFor efficient package management, we recommend uv ‚Äî a modern and fast tool for managing Python environments."
  },
  {
    "objectID": "doc_sources/install.html#method-1-install-from-source-for-development",
    "href": "doc_sources/install.html#method-1-install-from-source-for-development",
    "title": "Installation",
    "section": "Method 1: Install from Source (for Development)",
    "text": "Method 1: Install from Source (for Development)\nThis method is ideal if you want to contribute to the project or modify the code.\n\nClone the repository:\ngit clone https://github.com/aborruso/tometo_tomato.git\nNavigate to the project directory:\ncd tometo_tomato\nInstall dependencies:\n\nWith uv (recommended):\nuv sync\nThis creates a virtual environment and installs the package in editable mode, so code changes take effect immediately.\nWith pip:\npip install -e .\n\nIf you don‚Äôt plan to modify the code, you can omit the -e flag:\npip install ."
  },
  {
    "objectID": "doc_sources/install.html#method-2-install-as-a-global-tool-with-uv-tool-install",
    "href": "doc_sources/install.html#method-2-install-as-a-global-tool-with-uv-tool-install",
    "title": "Installation",
    "section": "Method 2: Install as a Global Tool (with uv tool install)",
    "text": "Method 2: Install as a Global Tool (with uv tool install)\nIf you just want to use tometo_tomato as a command-line tool without managing source code or virtual environments, uv tool install is the cleanest option.\n\nMake sure you have uv installed.\nInstall the tool:\nuv tool install tometo_tomato\nThis installs tometo_tomato in an isolated environment managed by uv and makes it available on your PATH."
  },
  {
    "objectID": "doc_sources/install.html#verify-the-installation",
    "href": "doc_sources/install.html#verify-the-installation",
    "title": "Installation",
    "section": "Verify the Installation",
    "text": "Verify the Installation\nAfter completing any of the installation methods, verify that tometo_tomato is installed correctly:\ntometo_tomato --version\nYou should see the version number printed on screen."
  },
  {
    "objectID": "doc_sources/use-case-guide.html",
    "href": "doc_sources/use-case-guide.html",
    "title": "Use Case Guide: Cleaning Dirty Data with a Reference Table",
    "section": "",
    "text": "You have a CSV file where one or more columns contain messy values: typos, missing characters, inconsistent capitalization. A typical example is a column of city or municipality names entered by hand.\nYou also have an authoritative reference file ‚Äî an official list with the correct names and additional attributes (such as codes or identifiers) that you need to bring into your working file.\nThe challenge: a regular join fails because the values do not match exactly. tometo_tomato solves this with a fuzzy join ‚Äî it finds the best approximate match for each row, even when the text is not identical."
  },
  {
    "objectID": "doc_sources/use-case-guide.html#the-problem",
    "href": "doc_sources/use-case-guide.html#the-problem",
    "title": "Use Case Guide: Cleaning Dirty Data with a Reference Table",
    "section": "",
    "text": "You have a CSV file where one or more columns contain messy values: typos, missing characters, inconsistent capitalization. A typical example is a column of city or municipality names entered by hand.\nYou also have an authoritative reference file ‚Äî an official list with the correct names and additional attributes (such as codes or identifiers) that you need to bring into your working file.\nThe challenge: a regular join fails because the values do not match exactly. tometo_tomato solves this with a fuzzy join ‚Äî it finds the best approximate match for each row, even when the text is not identical."
  },
  {
    "objectID": "doc_sources/use-case-guide.html#the-workflow",
    "href": "doc_sources/use-case-guide.html#the-workflow",
    "title": "Use Case Guide: Cleaning Dirty Data with a Reference Table",
    "section": "The Workflow",
    "text": "The Workflow\nThe process has two steps:\n Your file          Reference file\n (dirty data)       (ground truth)\n     ‚îÇ                    ‚îÇ\n     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ tometo_tomato ‚îÄ‚îò\n                ‚îÇ\n          Mapping table\n          (dirty ‚Üí clean)\n                ‚îÇ\n     ‚îå‚îÄ‚îÄ exact join (duckdb) ‚îÄ‚îÄ‚îê\n     ‚îÇ                         ‚îÇ\n Your file               Corrected file\n (original)              (enriched)\nStep 1. Run tometo_tomato to produce a mapping table that links each dirty value to its best match in the reference file.\nStep 2. Use a standard exact join (for example with DuckDB) to bring the corrections back into your original file."
  },
  {
    "objectID": "doc_sources/use-case-guide.html#example-a-single-join-column",
    "href": "doc_sources/use-case-guide.html#example-a-single-join-column",
    "title": "Use Case Guide: Cleaning Dirty Data with a Reference Table",
    "section": "Example A: Single Join Column",
    "text": "Example A: Single Join Column\n\nThe data\nYour working file (work_simple.csv):\n\n\n\nmunicipality\namount\n\n\n\n\nRoma\n1500\n\n\nRma\n2300\n\n\nMilno\n800\n\n\nMilano\n1200\n\n\nNpoli\n950\n\n\n\nReference file (reference_simple.csv):\n\n\n\nmunicipality_name\nistat_code\n\n\n\n\nRoma\n058091\n\n\nMilano\n015146\n\n\nNapoli\n063049\n\n\n\n\n\nStep 1: Generate the mapping\ntometo_tomato work_simple.csv reference_simple.csv \\\n  -j \"municipality,municipality_name\" \\\n  -a istat_code \\\n  -s -t 70 \\\n  -o mapping.csv\nFlags explained:\n\n-j \"municipality,municipality_name\" ‚Äî the pair of columns to compare (input column, reference column)\n-a istat_code ‚Äî an extra column from the reference file to include in the output\n-s ‚Äî show the similarity score\n-t 70 ‚Äî minimum similarity threshold (0-100)\n-o mapping.csv ‚Äî output file path\n\nResult (mapping.csv):\n\n\n\nmunicipality\nref_municipality_name\nistat_code\navg_score\n\n\n\n\nRoma\nRoma\n058091\n100.0\n\n\nRma\nRoma\n058091\n85.7\n\n\nMilano\nMilano\n015146\n100.0\n\n\nMilno\nMilano\n015146\n90.9\n\n\nNpoli\nNapoli\n063049\n90.9\n\n\n\nEach row in the mapping connects a dirty value to the best matching reference value. The avg_score column shows how confident the match is (100 = perfect).\n\n\nStep 2: Join back to your original file\nduckdb -c \"\nCOPY (\n  SELECT\n    w.*,\n    m.ref_municipality_name AS municipality_correct,\n    m.istat_code,\n    ROUND(m.avg_score, 1) AS score\n  FROM read_csv_auto('work_simple.csv') w\n  JOIN read_csv_auto('mapping.csv') m\n    ON w.municipality = m.municipality\n  ORDER BY w.id\n) TO 'work_corrected.csv' (HEADER);\n\"\nThe join condition is w.municipality = m.municipality ‚Äî an exact join on the dirty column. This works because the mapping table contains exactly the original (dirty) values from your file.\nResult (work_corrected.csv):\n\n\n\nmunicipality\namount\nmunicipality_correct\nistat_code\nscore\n\n\n\n\nRoma\n1500\nRoma\n058091\n100.0\n\n\nRma\n2300\nRoma\n058091\n85.7\n\n\nMilno\n800\nMilano\n015146\n90.9\n\n\nMilano\n1200\nMilano\n015146\n100.0\n\n\nNpoli\n950\nNapoli\n063049\n90.9\n\n\n\nEvery row in your original file now has the correct municipality name and its ISTAT code."
  },
  {
    "objectID": "doc_sources/use-case-guide.html#example-b-multi-column-join-disambiguation",
    "href": "doc_sources/use-case-guide.html#example-b-multi-column-join-disambiguation",
    "title": "Use Case Guide: Cleaning Dirty Data with a Reference Table",
    "section": "Example B: Multi-Column Join (Disambiguation)",
    "text": "Example B: Multi-Column Join (Disambiguation)\n\nWhy a single column is not always enough\nSome municipality names exist in multiple regions. For example, in Italy:\n\n\n\nMunicipality\nRegion\nISTAT Code\n\n\n\n\nCastro\nPuglia\n075019\n\n\nCastro\nLombardia\n016065\n\n\nSamone\nTrentino-Alto Adige\n022164\n\n\nSamone\nPiemonte\n001244\n\n\n\nIf you match only on the municipality name, ‚ÄúCastro‚Äù in Puglia could be incorrectly linked to ‚ÄúCastro‚Äù in Lombardia. You need a second column ‚Äî the region ‚Äî to disambiguate.\n\n\nThe data\nYour working file (work_file.csv):\nid,municipality,region,amount\n1,San Valentino in Abruzzo Citeriore,Abrzzo,1500\n2,San Valentino Torio,Campnia,2300\n3,Castro,Pugla,800\n4,Castro,Lombardia,1200\n5,Samone,Trntino-Alto Adige,950\n6,Samone,Piemonte,3100\n7,Calliano,Trentno-Alto Adige,400\n8,Calliano,Piemnte,2750\n9,Livo,Trentno-Alto Adige,1800\n10,Livo,Lombardia,600\nNotice the typos in the region column: ‚ÄúAbrzzo‚Äù, ‚ÄúCampnia‚Äù, ‚ÄúPugla‚Äù, ‚ÄúTrntino-Alto Adige‚Äù, ‚ÄúPiemnte‚Äù.\nReference file (reference.csv):\nmunicipality_name,region,istat_code\nSan Valentino in Abruzzo Citeriore,Abruzzo,068037\nSan Valentino Torio,Campania,065131\nCastro,Puglia,075019\nCastro,Lombardia,016065\nSamone,Trentino-Alto Adige,022164\nSamone,Piemonte,001244\nCalliano,Trentino-Alto Adige,022032\nCalliano,Piemonte,005013\nLivo,Trentino-Alto Adige,022107\nLivo,Lombardia,013130\n\n\nStep 1: Generate the mapping with two join pairs\ntometo_tomato work_file.csv reference.csv \\\n  -j \"municipality,municipality_name\" \\\n  -j \"region,region\" \\\n  -a istat_code \\\n  -s -t 70 \\\n  -o mapping.csv\nThe key difference: the -j flag is repeated for each column pair. The tool computes a similarity score for each pair and averages them. This means both the municipality name and the region must be similar for a match to succeed.\nResult (mapping.csv):\n\n\n\n\n\n\n\n\n\n\n\nmunicipality\nregion\nref_municipality_name\nref_region\nistat_code\navg_score\n\n\n\n\nCastro\nPugla\nCastro\nPuglia\n075019\n95.5\n\n\nCastro\nLombardia\nCastro\nLombardia\n016065\n100.0\n\n\nSamone\nTrntino-Alto Adige\nSamone\nTrentino-Alto Adige\n022164\n98.6\n\n\nSamone\nPiemonte\nSamone\nPiemonte\n001244\n100.0\n\n\nCalliano\nTrentno-Alto Adige\nCalliano\nTrentino-Alto Adige\n022032\n98.6\n\n\nCalliano\nPiemnte\nCalliano\nPiemonte\n005013\n96.7\n\n\n\n‚ÄúCastro, Pugla‚Äù correctly maps to ‚ÄúCastro, Puglia‚Äù (not Lombardia) because the averaged score on both columns picks the right region.\n\n\nStep 2: Join back using both columns\nduckdb -c \"\nCOPY (\n  SELECT\n    w.*,\n    m.ref_municipality_name AS municipality_correct,\n    m.ref_region AS region_correct,\n    m.istat_code,\n    ROUND(m.avg_score, 1) AS score\n  FROM read_csv_auto('work_file.csv') w\n  JOIN read_csv_auto('mapping.csv') m\n    ON w.municipality = m.municipality\n    AND w.region = m.region\n  ORDER BY w.id::INT\n) TO 'work_corrected.csv' (HEADER);\n\"\nThe join now uses both dirty columns: ON w.municipality = m.municipality AND w.region = m.region. This ensures each row maps to exactly one entry in the mapping table.\nResult (work_corrected.csv):\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nmunicipality\nregion\namount\nmunicipality_correct\nregion_correct\nistat_code\nscore\n\n\n\n\n1\nSan Valentino in Abruzzo Citeriore\nAbrzzo\n1500\nSan Valentino in Abruzzo Citeriore\nAbruzzo\n068037\n96.2\n\n\n2\nSan Valentino Torio\nCampnia\n2300\nSan Valentino Torio\nCampania\n065131\n96.7\n\n\n3\nCastro\nPugla\n800\nCastro\nPuglia\n075019\n95.5\n\n\n4\nCastro\nLombardia\n1200\nCastro\nLombardia\n016065\n100.0\n\n\n5\nSamone\nTrntino-Alto Adige\n950\nSamone\nTrentino-Alto Adige\n022164\n98.6\n\n\n6\nSamone\nPiemonte\n3100\nSamone\nPiemonte\n001244\n100.0\n\n\n7\nCalliano\nTrentno-Alto Adige\n400\nCalliano\nTrentino-Alto Adige\n022032\n98.6\n\n\n8\nCalliano\nPiemnte\n2750\nCalliano\nPiemonte\n005013\n96.7\n\n\n9\nLivo\nTrentno-Alto Adige\n1800\nLivo\nTrentino-Alto Adige\n022107\n98.6\n\n\n10\nLivo\nLombardia\n600\nLivo\nLombardia\n013130\n100.0\n\n\n\nAll 10 rows are correctly matched, with the right ISTAT code for each municipality-region pair."
  },
  {
    "objectID": "doc_sources/use-case-guide.html#tips-and-best-practices",
    "href": "doc_sources/use-case-guide.html#tips-and-best-practices",
    "title": "Use Case Guide: Cleaning Dirty Data with a Reference Table",
    "section": "Tips and Best Practices",
    "text": "Tips and Best Practices\n\nReviewing low-confidence matches\nUse the score column to spot matches that might be wrong. A quick check:\nduckdb -c \"\nSELECT * FROM read_csv_auto('mapping.csv')\nWHERE avg_score &lt; 95\nORDER BY avg_score;\n\"\nRows with lower scores deserve manual review.\n\n\nHandling ambiguous matches\nWhen two reference rows score equally well for the same input, tometo_tomato flags the match as ambiguous and excludes it from the clean output to avoid inserting incorrect data. Use --output-ambiguous to inspect these cases:\ntometo_tomato work_file.csv reference.csv \\\n  -j \"municipality,municipality_name\" \\\n  -j \"region,region\" \\\n  -a istat_code \\\n  -s -t 70 \\\n  -o mapping.csv \\\n  -u ambiguous.csv\n\n\nUnmatched rows\nIf an input row has no reference match above the threshold, it still appears in the mapping file ‚Äî but with empty reference columns. This is a LEFT JOIN: no input row is ever lost. You can filter these out in Step 2 or handle them separately.\n\n\nChoosing the right threshold\n\n90-100: strict, only very close matches (good for clean data with minor typos)\n80-90: moderate, handles abbreviations and missing characters\n70-80: permissive, catches more distant matches but needs manual review\n\n\n\nPerformance on large datasets\nFor large files, use --block-prefix N to avoid comparing every pair of rows:\ntometo_tomato work_file.csv reference.csv \\\n  -j \"municipality,municipality_name\" \\\n  -j \"region,region\" \\\n  -a istat_code \\\n  --block-prefix 3 \\\n  -s -t 70 \\\n  -o mapping.csv\nThis only compares rows where the first 3 characters of the join columns match, dramatically reducing computation time."
  }
]